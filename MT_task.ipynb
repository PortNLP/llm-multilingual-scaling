{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zero-shot",
   "id": "881ca06268d1761f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ZeroShot learning for SIB-200 dataset\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "model_address = \"bigscience/bloom-1b1\"\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address, padding_side='left')\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to translate given texts to English\n",
    "def translate_batch(texts, input_language, output_language=\"English\", batch_size=5):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        generated_texts = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            prompts = [f\"{input_language}: {text} \\n{output_language}:\" for text in batch_texts]\n",
    "            inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokens = tokenizer.batch_encode_plus(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "            result_length = tokens.shape[1] + 100\n",
    "            generated_batch = model.generate(inputs[\"input_ids\"], max_length=result_length)\n",
    "            for generated_text in generated_batch:\n",
    "                generated_texts.append(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    return generated_texts\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"MT-task/sib-200/zero-shot/\" + model_address[model_address.find('/')+1:] + \"/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "language_df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Iterate through rows and compare predicted category with actual category\n",
    "for index, row in language_df.iterrows():\n",
    "    language = row['Language Name']\n",
    "    folder = row['Folder Name']\n",
    "    print(language)\n",
    "    if language == \"English\":\n",
    "        continue\n",
    "    if f\"{folder}.csv\" in os.listdir(output_directory):\n",
    "        continue\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'translated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, folder)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            # Check if the file is already present\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            if os.path.exists(results_file_path):\n",
    "                print(f\"Output file {results_file_path} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Get all texts from the DataFrame\n",
    "            texts = df['text'].tolist()\n",
    "\n",
    "            # Predict translations using your ZeroShot learning model in batches\n",
    "            generated_texts = translate_batch(texts=texts, input_language=language)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df['text'] = texts\n",
    "            results_df['translated_text'] = generated_texts\n",
    "\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "id": "9dfa01d5aaada82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Few-shot",
   "id": "bb76a7df8bc52e51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T02:14:22.992191Z",
     "start_time": "2024-04-20T22:14:28.516455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## FewShot learning for SIB-200 dataset\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "model_address = \"bigscience/bloom-560m\"\n",
    "n_shot = 2\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address, padding_side='left')\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "def few_shot_maker(input_folder, input_language, output_folder=\"eng_Latn\", output_language=\"English\", n_shots=2):\n",
    "    # Read the few-shot samples from the input folder\n",
    "    input_folder_path = os.path.join(data_directory, input_folder)\n",
    "    output_folder_path = os.path.join(data_directory, output_folder)\n",
    "    input_df = pd.read_csv(os.path.join(input_folder_path, \"train.tsv\"), sep='\\t')\n",
    "    output_df = pd.read_csv(os.path.join(output_folder_path, \"train.tsv\"), sep='\\t')\n",
    "    few_shot = \"\\n\\n\".join([f\"{input_language}: {input_df['text'][i]} \\n{output_language}: {output_df['text'][i]}\" for i in range(n_shots)])\n",
    "    return few_shot\n",
    "\n",
    "# Function to translate given texts to English\n",
    "def translate_batch(texts, few_shot_sample, input_language, output_language=\"English\", batch_size=10):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        generated_texts = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            prompts = [f\"{few_shot_sample}\\n\\n{input_language}: {text} \\n{output_language}:\" for text in batch_texts]\n",
    "            inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokens = tokenizer.batch_encode_plus(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "            result_length = tokens.shape[1] + 100\n",
    "            generated_batch = model.generate(inputs[\"input_ids\"], max_length=result_length)\n",
    "            for generated_text in generated_batch:\n",
    "                generated_texts.append(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    return generated_texts\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = f\"MT-task/sib-200/{n_shot}-shot/\" + model_address[model_address.find('/')+1:] + \"/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "language_df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Iterate through rows and compare predicted category with actual category\n",
    "for index, row in language_df.iterrows():\n",
    "    language = row['Language Name']\n",
    "    folder = row['Folder Name']\n",
    "    print(language)\n",
    "    if language == \"English\":\n",
    "        continue\n",
    "    if f\"{folder}.csv\" in os.listdir(output_directory):\n",
    "        continue\n",
    "    few_shot_sample = few_shot_maker(folder, language, n_shots=n_shot)    \n",
    "    \n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'translated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, folder)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Check if the file is already present\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            if os.path.exists(results_file_path):\n",
    "                print(f\"Output file {results_file_path} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Get all texts from the DataFrame\n",
    "            texts = df['text'].tolist()\n",
    "\n",
    "            # Predict translations using your ZeroShot learning model in batches\n",
    "            generated_texts = translate_batch(texts=texts, few_shot_sample=few_shot_sample, input_language=language)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df['text'] = texts\n",
    "            results_df['translated_text'] = generated_texts\n",
    "\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "id": "cf830aee3ac19c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aceh\n",
      "Aceh\n",
      "Mesopotamian Spoken Arabic\n",
      "Arabic, Ta’izzi-Adeni Spoken\n",
      "Tunisian Spoken Arabic\n",
      "Afrikaans\n",
      "South Levantine Arabic\n",
      "Akan\n",
      "Tosk Albanian\n",
      "Amharic\n",
      "Levantine Arabic\n",
      "Standard Arabic\n",
      "Standard Arabic\n",
      "Najdi Spoken Arabic\n",
      "Moroccan Spoken Arabic\n",
      "Egyptian Spoken Arabic\n",
      "Assamese\n",
      "Asturianu\n",
      "Awadhi\n",
      "Central Aymara\n",
      "South Azerbaijani\n",
      "North Azerbaijani\n",
      "Bashkort\n",
      "Bamanankan\n",
      "Bali\n",
      "Belarusian\n",
      "Bemba\n",
      "Bengali\n",
      "Bhojpuri\n",
      "Banjar\n",
      "Banjar\n",
      "Central Tibetan\n",
      "Bosnian\n",
      "Bugis\n",
      "Bulgarian\n",
      "Catalan\n",
      "Cebuano\n",
      "Czech\n",
      "Chokwe\n",
      "Central Kurdish\n",
      "Crimean Tatar\n",
      "Welsh\n",
      "Danish\n",
      "Standard German\n",
      "Southwestern Dinka\n",
      "Jula\n",
      "Dzongkha\n",
      "Greek\n",
      "English\n",
      "Esperanto\n",
      "Estonian\n",
      "Euskera\n",
      "Éwé\n",
      "Faroese\n",
      "Fijian\n",
      "Finnish\n",
      "Fon\n",
      "French\n",
      "Friulian\n",
      "Nigerian Fulfulde\n",
      "West Central Oromo\n",
      "Scottish Gaelic\n",
      "Irish\n",
      "Galician\n",
      "Guarani\n",
      "Gujarati\n",
      "Haitian Creole\n",
      "Hausa\n",
      "Hebrew\n",
      "Hindi\n",
      "Chhattisgarhi\n",
      "Croatian\n",
      "Hungarian\n",
      "Armenian\n",
      "Igbo\n",
      "Ilocano\n",
      "Indonesian\n",
      "Icelandic\n",
      "Italian\n",
      "Javanese\n",
      "Japanese\n",
      "Kabyle\n",
      "Jingpho\n",
      "Kamba\n",
      "Kannada\n",
      "Kashmiri\n",
      "Kashmiri\n",
      "Georgian\n",
      "Kazakh\n",
      "Kabiyè\n",
      "Kabuverdianu\n",
      "Halh Mongolian\n",
      "Khmer\n",
      "Gikuyu\n",
      "Kinyarwanda\n",
      "Kyrgyz\n",
      "Kimbundu\n",
      "Northern Kurdish\n",
      "Yerwa Kanuri\n",
      "Yerwa Kanuri\n",
      "Kongo\n",
      "Korean\n",
      "Lao\n",
      "Ligurian\n",
      "Limburgish\n",
      "Lingala\n",
      "Lithuanian\n",
      "Lombard\n",
      "Latgalian\n",
      "Luxembourgish\n",
      "Luba-Lulua\n",
      "Ganda\n",
      "Dholuo\n",
      "Mizo\n",
      "Standard Latvian\n",
      "Magahi\n",
      "Maithili\n",
      "Malayalam\n",
      "Marathi\n",
      "Minangkabau\n",
      "Minangkabau\n",
      "Macedonian\n",
      "Maltese\n",
      "Meitei\n",
      "Moore\n",
      "Maori\n",
      "Burmese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/mya_Mymr.csv\n",
      "Dutch\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nld_Latn.csv\n",
      "Nynorsk\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nno_Latn.csv\n",
      "Bokmål\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nob_Latn.csv\n",
      "Nepali\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/npi_Deva.csv\n",
      "Northern Sotho\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nso_Latn.csv\n",
      "Nuer\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nus_Latn.csv\n",
      "Chichewa\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/nya_Latn.csv\n",
      "Occitan\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/oci_Latn.csv\n",
      "Odia\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/ory_Orya.csv\n",
      "Pangasinan\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/pag_Latn.csv\n",
      "Eastern Punjabi\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/pan_Guru.csv\n",
      "Papiamentu\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/pap_Latn.csv\n",
      "Southern Pashto\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/pbt_Arab.csv\n",
      "Iranian Persian\n",
      "Merina Malagasy\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/plt_Latn.csv\n",
      "Polish\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/pol_Latn.csv\n",
      "Portuguese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/por_Latn.csv\n",
      "Dari\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/prs_Arab.csv\n",
      "Ayacucho Quechua\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/quy_Latn.csv\n",
      "Romanian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/ron_Latn.csv\n",
      "Rundi\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/run_Latn.csv\n",
      "Russian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/rus_Cyrl.csv\n",
      "Sango\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sag_Latn.csv\n",
      "Sanskrit\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/san_Deva.csv\n",
      "Santhali\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sat_Olck.csv\n",
      "Sicilian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/scn_Latn.csv\n",
      "Shan\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/shn_Mymr.csv\n",
      "Sinhala\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sin_Sinh.csv\n",
      "Slovak\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/slk_Latn.csv\n",
      "Slovene\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/slv_Latn.csv\n",
      "Samoan\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/smo_Latn.csv\n",
      "Shona\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sna_Latn.csv\n",
      "Sindhi\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/snd_Arab.csv\n",
      "Somali\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/som_Latn.csv\n",
      "Southern Sotho\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sot_Latn.csv\n",
      "Spanish\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/spa_Latn.csv\n",
      "Sardinian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/srd_Latn.csv\n",
      "Serbian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/srp_Cyrl.csv\n",
      "Swati\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/ssw_Latn.csv\n",
      "Sunda\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/sun_Latn.csv\n",
      "Swedish\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/swe_Latn.csv\n",
      "Swahili\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/swh_Latn.csv\n",
      "Silesian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/szl_Latn.csv\n",
      "Tamil\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tam_Taml.csv\n",
      "Tamasheq\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/taq_Latn.csv\n",
      "Tamasheq\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/taq_Tfng.csv\n",
      "Tatar\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tat_Cyrl.csv\n",
      "Telugu\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tel_Telu.csv\n",
      "Tajik\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tgk_Cyrl.csv\n",
      "Tagalog\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tgl_Latn.csv\n",
      "Thai\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tha_Thai.csv\n",
      "Tigrigna\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tir_Ethi.csv\n",
      "Tok Pisin\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tpi_Latn.csv\n",
      "Setswana\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tsn_Latn.csv\n",
      "Tsonga\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tso_Latn.csv\n",
      "Turkmen\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tuk_Latn.csv\n",
      "Tumbuka\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tum_Latn.csv\n",
      "Turkish\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tur_Latn.csv\n",
      "Twi\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/twi_Latn.csv\n",
      "Central Atlas Tamazight\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/tzm_Tfng.csv\n",
      "Uyghur\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/uig_Arab.csv\n",
      "Ukrainian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/ukr_Cyrl.csv\n",
      "Umbundu\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/umb_Latn.csv\n",
      "Urdu\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/urd_Arab.csv\n",
      "Northern Uzbek\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/uzn_Latn.csv\n",
      "Venetian\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/vec_Latn.csv\n",
      "Vietnamese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/vie_Latn.csv\n",
      "Waray-Waray\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/war_Latn.csv\n",
      "Wolof\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/wol_Latn.csv\n",
      "Xhosa\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/xho_Latn.csv\n",
      "Eastern Yiddish\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/ydd_Hebr.csv\n",
      "Yoruba\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/yor_Latn.csv\n",
      "Yue Chinese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/yue_Hant.csv\n",
      "Chinese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/zho_Hans.csv\n",
      "Chinese\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/zho_Hant.csv\n",
      "Standard Malay\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/zsm_Latn.csv\n",
      "Zulu\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-560m/zul_Latn.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eebcbb76b7ddeb6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
