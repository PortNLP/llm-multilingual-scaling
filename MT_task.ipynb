{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zero-shot",
   "id": "881ca06268d1761f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ZeroShot learning for SIB-200 dataset\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "model_address = \"bigscience/bloom-1b1\"\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address, padding_side='left')\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to translate given texts to English\n",
    "def translate_batch(texts, input_language, output_language=\"English\", batch_size=5):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        generated_texts = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            prompts = [f\"{input_language}: {text} \\n{output_language}:\" for text in batch_texts]\n",
    "            inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokens = tokenizer.batch_encode_plus(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "            result_length = tokens.shape[1] + 100\n",
    "            generated_batch = model.generate(inputs[\"input_ids\"], max_length=result_length)\n",
    "            for generated_text in generated_batch:\n",
    "                generated_texts.append(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    return generated_texts\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"MT-task/sib-200/zero-shot/\" + model_address[model_address.find('/')+1:] + \"/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "language_df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Iterate through rows and compare predicted category with actual category\n",
    "for index, row in language_df.iterrows():\n",
    "    language = row['Language Name']\n",
    "    folder = row['Folder Name']\n",
    "    print(language)\n",
    "    if language == \"English\":\n",
    "        continue\n",
    "    if f\"{folder}.csv\" in os.listdir(output_directory):\n",
    "        continue\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'translated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, folder)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            # Check if the file is already present\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            if os.path.exists(results_file_path):\n",
    "                print(f\"Output file {results_file_path} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Get all texts from the DataFrame\n",
    "            texts = df['text'].tolist()\n",
    "\n",
    "            # Predict translations using your ZeroShot learning model in batches\n",
    "            generated_texts = translate_batch(texts=texts, input_language=language)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df['text'] = texts\n",
    "            results_df['translated_text'] = generated_texts\n",
    "\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "id": "9dfa01d5aaada82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Few-shot",
   "id": "bb76a7df8bc52e51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T04:15:08.784072Z",
     "start_time": "2024-04-21T04:05:30.783816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## FewShot learning for SIB-200 dataset\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "model_address = \"bigscience/bloom-1b1\"\n",
    "n_shot = 2\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address, padding_side='left')\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "def few_shot_maker(input_folder, input_language, output_folder=\"eng_Latn\", output_language=\"English\", n_shots=2):\n",
    "    # Read the few-shot samples from the input folder\n",
    "    input_folder_path = os.path.join(data_directory, input_folder)\n",
    "    output_folder_path = os.path.join(data_directory, output_folder)\n",
    "    input_df = pd.read_csv(os.path.join(input_folder_path, \"train.tsv\"), sep='\\t')\n",
    "    output_df = pd.read_csv(os.path.join(output_folder_path, \"train.tsv\"), sep='\\t')\n",
    "    few_shot = \"\\n\\n\".join([f\"{input_language}: {input_df['text'][i]} \\n{output_language}: {output_df['text'][i]}\" for i in range(n_shots)])\n",
    "    return few_shot\n",
    "\n",
    "# Function to translate given texts to English\n",
    "def translate_batch(texts, few_shot_sample, input_language, output_language=\"English\", batch_size=16):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        generated_texts = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            prompts = [f\"{few_shot_sample}\\n\\n{input_language}: {text} \\n{output_language}:\" for text in batch_texts]\n",
    "            inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokens = tokenizer.batch_encode_plus(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "            result_length = tokens.shape[1] + 100\n",
    "            generated_batch = model.generate(inputs[\"input_ids\"], max_length=result_length)\n",
    "            for generated_text in generated_batch:\n",
    "                generated_texts.append(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    return generated_texts\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = f\"MT-task/sib-200/{n_shot}-shot/\" + model_address[model_address.find('/')+1:] + \"/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "language_df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Iterate through rows and compare predicted category with actual category\n",
    "for index, row in language_df.iterrows():\n",
    "    language = row['Language Name']\n",
    "    folder = row['Folder Name']\n",
    "    print(language)\n",
    "    if language == \"English\":\n",
    "        continue\n",
    "    if f\"{folder}.csv\" in os.listdir(output_directory):\n",
    "        continue\n",
    "    few_shot_sample = few_shot_maker(folder, language, n_shots=n_shot)    \n",
    "    \n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'translated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, folder)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Check if the file is already present\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            if os.path.exists(results_file_path):\n",
    "                print(f\"Output file {results_file_path} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Get all texts from the DataFrame\n",
    "            texts = df['text'].tolist()\n",
    "\n",
    "            # Predict translations using your ZeroShot learning model in batches\n",
    "            generated_texts = translate_batch(texts=texts, few_shot_sample=few_shot_sample, input_language=language)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df['text'] = texts\n",
    "            results_df['translated_text'] = generated_texts\n",
    "\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "id": "cf830aee3ac19c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aceh\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-1b1/ace_Arab.csv\n",
      "Aceh\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-1b1/ace_Latn.csv\n",
      "Mesopotamian Spoken Arabic\n",
      "Results saved to MT-task/sib-200/2-shot/bloom-1b1/acm_Arab.csv\n",
      "Arabic, Ta’izzi-Adeni Spoken\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 100\u001B[0m\n\u001B[1;32m     97\u001B[0m texts \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Predict translations using your ZeroShot learning model in batches\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m generated_texts \u001B[38;5;241m=\u001B[39m translate_batch(texts\u001B[38;5;241m=\u001B[39mtexts, few_shot_sample\u001B[38;5;241m=\u001B[39mfew_shot_sample, input_language\u001B[38;5;241m=\u001B[39mlanguage)\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# Append the results to the DataFrame\u001B[39;00m\n\u001B[1;32m    103\u001B[0m results_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m texts\n",
      "Cell \u001B[0;32mIn[1], line 52\u001B[0m, in \u001B[0;36mtranslate_batch\u001B[0;34m(texts, few_shot_sample, input_language, output_language, batch_size)\u001B[0m\n\u001B[1;32m     50\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_encode_plus(prompts, add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     51\u001B[0m result_length \u001B[38;5;241m=\u001B[39m tokens\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m---> 52\u001B[0m generated_batch \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m], max_length\u001B[38;5;241m=\u001B[39mresult_length)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m generated_text \u001B[38;5;129;01min\u001B[39;00m generated_batch:\n\u001B[1;32m     54\u001B[0m     generated_texts\u001B[38;5;241m.\u001B[39mappend(tokenizer\u001B[38;5;241m.\u001B[39mdecode(generated_text, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[0;32m~/anaconda3/envs/mbbc/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/mbbc/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1526\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[1;32m   1527\u001B[0m         input_ids,\n\u001B[1;32m   1528\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1541\u001B[0m     )\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[1;32m   1543\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1544\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgreedy_search(\n\u001B[1;32m   1545\u001B[0m         input_ids,\n\u001B[1;32m   1546\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mprepared_logits_processor,\n\u001B[1;32m   1547\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mprepared_stopping_criteria,\n\u001B[1;32m   1548\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[1;32m   1549\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[1;32m   1550\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[1;32m   1551\u001B[0m         output_logits\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_logits,\n\u001B[1;32m   1552\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[1;32m   1553\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[1;32m   1554\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[1;32m   1555\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1556\u001B[0m     )\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[1;32m   1559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/anaconda3/envs/mbbc/lib/python3.11/site-packages/transformers/generation/utils.py:2467\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2464\u001B[0m         this_peer_finished \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2466\u001B[0m \u001B[38;5;66;03m# stop if we exceed the maximum length\u001B[39;00m\n\u001B[0;32m-> 2467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stopping_criteria(input_ids, scores):\n\u001B[1;32m   2468\u001B[0m     this_peer_finished \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m this_peer_finished \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m synced_gpus:\n",
      "File \u001B[0;32m~/anaconda3/envs/mbbc/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:130\u001B[0m, in \u001B[0;36mStoppingCriteriaList.__call__\u001B[0;34m(self, input_ids, scores, **kwargs)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mStoppingCriteriaList\u001B[39;00m(\u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;129m@add_start_docstrings\u001B[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids: torch\u001B[38;5;241m.\u001B[39mLongTensor, scores: torch\u001B[38;5;241m.\u001B[39mFloatTensor, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28many\u001B[39m(criteria(input_ids, scores, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mfor\u001B[39;00m criteria \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_length\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mint\u001B[39m]:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eebcbb76b7ddeb6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
