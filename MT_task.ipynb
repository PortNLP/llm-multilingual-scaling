{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-19T23:41:43.259796Z"
    }
   },
   "source": [
    "## ZeroShot learning for SIB-200 dataset by generating text\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "model_address = \"bigscience/bloom-1b1\"\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address, padding_side='left')\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to translate given texts to English\n",
    "def translate_batch(texts, input_language, output_language=\"English\", batch_size=5):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        generated_texts = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            prompts = [f\"{input_language}: {text} \\n{output_language}:\" for text in batch_texts]\n",
    "            inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "            tokens = tokenizer.batch_encode_plus(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\")['input_ids']\n",
    "            result_length = tokens.shape[1] + 100\n",
    "            generated_batch = model.generate(inputs[\"input_ids\"], max_length=result_length)\n",
    "            for generated_text in generated_batch:\n",
    "                generated_texts.append(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    return generated_texts\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"MT-task/sib-200/\" + model_address[model_address.find('/')+1:] + \"/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "language_df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Iterate through rows and compare predicted category with actual category\n",
    "for index, row in language_df.iterrows():\n",
    "    language = row['Language Name']\n",
    "    folder = row['Folder Name']\n",
    "    print(language)\n",
    "    if language == \"English\":\n",
    "        continue\n",
    "    if f\"{folder}.csv\" in os.listdir(output_directory):\n",
    "        continue\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'translated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, folder)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            # Check if the file is already present\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            if os.path.exists(results_file_path):\n",
    "                print(f\"Output file {results_file_path} already exists. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Get all texts from the DataFrame\n",
    "            texts = df['text'].tolist()\n",
    "\n",
    "            # Predict translations using your ZeroShot learning model in batches\n",
    "            generated_texts = translate_batch(texts=texts, input_language=language)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            results_df['text'] = texts\n",
    "            results_df['translated_text'] = generated_texts\n",
    "\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{folder}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aceh\n",
      "Aceh\n",
      "Mesopotamian Spoken Arabic\n",
      "Arabic, Ta’izzi-Adeni Spoken\n",
      "Tunisian Spoken Arabic\n",
      "Afrikaans\n",
      "South Levantine Arabic\n",
      "Akan\n",
      "Tosk Albanian\n",
      "Amharic\n",
      "Levantine Arabic\n",
      "Standard Arabic\n",
      "Standard Arabic\n",
      "Najdi Spoken Arabic\n",
      "Moroccan Spoken Arabic\n",
      "Egyptian Spoken Arabic\n",
      "Assamese\n",
      "Asturianu\n",
      "Awadhi\n",
      "Central Aymara\n",
      "South Azerbaijani\n",
      "North Azerbaijani\n",
      "Bashkort\n",
      "Bamanankan\n",
      "Bali\n",
      "Belarusian\n",
      "Bemba\n",
      "Bengali\n",
      "Bhojpuri\n",
      "Banjar\n",
      "Banjar\n",
      "Central Tibetan\n",
      "Bosnian\n",
      "Bugis\n",
      "Bulgarian\n",
      "Catalan\n",
      "Cebuano\n",
      "Czech\n",
      "Chokwe\n",
      "Central Kurdish\n",
      "Crimean Tatar\n",
      "Welsh\n",
      "Danish\n",
      "Standard German\n",
      "Southwestern Dinka\n",
      "Jula\n",
      "Dzongkha\n",
      "Greek\n",
      "English\n",
      "Esperanto\n",
      "Estonian\n",
      "Euskera\n",
      "Éwé\n",
      "Faroese\n",
      "Fijian\n",
      "Finnish\n",
      "Fon\n",
      "French\n",
      "Friulian\n",
      "Nigerian Fulfulde\n",
      "West Central Oromo\n",
      "Scottish Gaelic\n",
      "Irish\n",
      "Galician\n",
      "Guarani\n",
      "Gujarati\n",
      "Haitian Creole\n",
      "Hausa\n",
      "Hebrew\n",
      "Hindi\n",
      "Chhattisgarhi\n",
      "Croatian\n",
      "Hungarian\n",
      "Armenian\n",
      "Igbo\n",
      "Ilocano\n",
      "Indonesian\n",
      "Icelandic\n",
      "Italian\n",
      "Javanese\n",
      "Japanese\n",
      "Kabyle\n",
      "Jingpho\n",
      "Kamba\n",
      "Kannada\n",
      "Kashmiri\n",
      "Kashmiri\n",
      "Georgian\n",
      "Kazakh\n",
      "Kabiyè\n",
      "Kabuverdianu\n",
      "Halh Mongolian\n",
      "Khmer\n",
      "Gikuyu\n",
      "Kinyarwanda\n",
      "Kyrgyz\n",
      "Kimbundu\n",
      "Northern Kurdish\n",
      "Yerwa Kanuri\n",
      "Yerwa Kanuri\n",
      "Kongo\n",
      "Korean\n",
      "Lao\n",
      "Ligurian\n",
      "Limburgish\n",
      "Lingala\n",
      "Lithuanian\n",
      "Lombard\n",
      "Latgalian\n",
      "Luxembourgish\n",
      "Luba-Lulua\n",
      "Ganda\n",
      "Dholuo\n",
      "Mizo\n",
      "Standard Latvian\n",
      "Magahi\n",
      "Maithili\n",
      "Malayalam\n",
      "Marathi\n",
      "Minangkabau\n",
      "Minangkabau\n",
      "Macedonian\n",
      "Maltese\n",
      "Meitei\n",
      "Moore\n",
      "Maori\n",
      "Burmese\n",
      "Dutch\n",
      "Nynorsk\n",
      "Bokmål\n",
      "Nepali\n",
      "Northern Sotho\n",
      "Nuer\n",
      "Chichewa\n",
      "Occitan\n",
      "Odia\n",
      "Pangasinan\n",
      "Eastern Punjabi\n",
      "Papiamentu\n",
      "Southern Pashto\n",
      "Iranian Persian\n",
      "Merina Malagasy\n",
      "Polish\n",
      "Portuguese\n",
      "Dari\n",
      "Ayacucho Quechua\n",
      "Romanian\n",
      "Rundi\n",
      "Russian\n",
      "Sango\n",
      "Sanskrit\n",
      "Santhali\n",
      "Sicilian\n",
      "Shan\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc480fcb8364fc96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
