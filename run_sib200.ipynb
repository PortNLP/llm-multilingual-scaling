{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "    Folder Name    Language Name ISO 639 - 1 ISO 639 - 2 (B) ISO 639 - 2 (T)  \\\n7      aka_Latn             Akan          ak             aka             aka   \n11     arb_Arab  Standard Arabic         NaN             NaN             NaN   \n16     asm_Beng         Assamese          as             asm             asm   \n23     bam_Latn       Bamanankan          bm             bam             bam   \n27     ben_Beng          Bengali          bn             ben             ben   \n35     cat_Latn          Catalan          ca             cat             cat   \n48     eng_Latn          English          en             eng             eng   \n51     eus_Latn          Euskera          eu             baq             eus   \n56     fon_Latn              Fon         NaN             fon             fon   \n57     fra_Latn           French          fr             fre             fra   \n65     guj_Gujr         Gujarati          gu             guj             guj   \n69     hin_Deva            Hindi          hi             hin             hin   \n74     ibo_Latn             Igbo          ig             ibo             ibo   \n76     ind_Latn       Indonesian          id             ind             ind   \n84     kan_Knda          Kannada          kn             kan             kan   \n93     kik_Latn           Gikuyu          ki             kik             kik   \n94     kin_Latn      Kinyarwanda          rw             kin             kin   \n105    lin_Latn          Lingala          ln             lin             lin   \n111    lug_Latn            Ganda          lg             lug             lug   \n117    mal_Mlym        Malayalam          ml             mal             mal   \n118    mar_Deva          Marathi          mr             mar             mar   \n130    npi_Deva           Nepali          ne             NaN             NaN   \n131    nso_Latn   Northern Sotho         NaN             nso             nso   \n133    nya_Latn         Chichewa          ny             nya             nya   \n135    ory_Orya             Odia          or             NaN             NaN   \n137    pan_Guru  Eastern Punjabi          pa             pan             pan   \n143    por_Latn       Portuguese          pt             por             por   \n147    run_Latn            Rundi          rn             run             run   \n158    sna_Latn            Shona          sn             sna             sna   \n161    sot_Latn   Southern Sotho          st             sot             sot   \n162    spa_Latn          Spanish          es             spa             spa   \n168    swh_Latn          Swahili          sw             NaN             NaN   \n170    tam_Taml            Tamil          ta             tam             tam   \n174    tel_Telu           Telugu          te             tel             tel   \n180    tsn_Latn         Setswana          tn             tsn             tsn   \n181    tso_Latn           Tsonga          ts             tso             tso   \n183    tum_Latn          Tumbuka         NaN             tum             tum   \n190    urd_Arab             Urdu          ur             urd             urd   \n193    vie_Latn       Vietnamese          vi             vie             vie   \n195    wol_Latn            Wolof          wo             wol             wol   \n196    xho_Latn            Xhosa          xh             xho             xho   \n198    yor_Latn           Yoruba          yo             yor             yor   \n200    zho_Hans          Chinese          zh             chi             zho   \n201    zho_Hant          Chinese          zh             chi             zho   \n203    zul_Latn             Zulu          zu             zul             zul   \n\n    ISO 639 - 3 Script (ISO 15924) Language Family              Population  \\\n7           aka               Latn     Niger-Congo  1 million to 1 billion   \n11          arb               Arab    Afro-Asiatic  1 million to 1 billion   \n16          asm               Beng   Indo-European  1 million to 1 billion   \n23          bam               Latn     Niger-Congo  1 million to 1 billion   \n27          ben               Beng   Indo-European  1 million to 1 billion   \n35          cat               Latn   Indo-European  1 million to 1 billion   \n48          eng               Latn   Indo-European          1 billion plus   \n51          eus               Latn         Isolate  1 million to 1 billion   \n56          fon               Latn     Niger-Congo  1 million to 1 billion   \n57          fra               Latn   Indo-European  1 million to 1 billion   \n65          guj               Gujr   Indo-European  1 million to 1 billion   \n69          hin               Deva   Indo-European  1 million to 1 billion   \n74          ibo               Latn     Niger-Congo  1 million to 1 billion   \n76          ind               Latn    Austronesian  1 million to 1 billion   \n84          kan               Knda       Dravidian  1 million to 1 billion   \n93          kik               Latn     Niger-Congo  1 million to 1 billion   \n94          kin               Latn     Niger-Congo  1 million to 1 billion   \n105         lin               Latn     Niger-Congo  1 million to 1 billion   \n111         lug               Latn     Niger-Congo  1 million to 1 billion   \n117         mal               Mlym       Dravidian  1 million to 1 billion   \n118         mar               Deva   Indo-European  1 million to 1 billion   \n130         npi               Deva   Indo-European  1 million to 1 billion   \n131         nso               Latn     Niger-Congo  1 million to 1 billion   \n133         nya               Latn     Niger-Congo  1 million to 1 billion   \n135         ory               Orya   Indo-European  1 million to 1 billion   \n137         pan               Guru   Indo-European  1 million to 1 billion   \n143         por               Latn   Indo-European  1 million to 1 billion   \n147         run               Latn     Niger-Congo  1 million to 1 billion   \n158         sna               Latn     Niger-Congo  1 million to 1 billion   \n161         sot               Latn     Niger-Congo  1 million to 1 billion   \n162         spa               Latn   Indo-European  1 million to 1 billion   \n168         swh               Latn     Niger-Congo  1 million to 1 billion   \n170         tam               Taml       Dravidian  1 million to 1 billion   \n174         tel               Telu       Dravidian  1 million to 1 billion   \n180         tsn               Latn     Niger-Congo  1 million to 1 billion   \n181         tso               Latn     Niger-Congo  1 million to 1 billion   \n183         tum               Latn     Niger-Congo  1 million to 1 billion   \n190         urd               Arab   Indo-European  1 million to 1 billion   \n193         vie               Latn  Austro-Asiatic  1 million to 1 billion   \n195         wol               Latn     Niger-Congo  1 million to 1 billion   \n196         xho               Latn     Niger-Congo  1 million to 1 billion   \n198         yor               Latn     Niger-Congo  1 million to 1 billion   \n200         zho               Hans    Sino-Tibetan          1 billion plus   \n201         zho               Hant    Sino-Tibetan          1 billion plus   \n203         zul               Latn     Niger-Congo  1 million to 1 billion   \n\n    Language Vitality Digital Language Support Resource Level  \\\n7       Institutional                Ascending              1   \n11      Institutional                 Thriving              5   \n16      Institutional                    Vital              1   \n23      Institutional                Ascending              1   \n27      Institutional                    Vital              3   \n35      Institutional                 Thriving              4   \n48      Institutional                 Thriving              5   \n51      Institutional                    Vital              4   \n56      Institutional                 Emerging              0   \n57      Institutional                 Thriving              5   \n65      Institutional                    Vital              1   \n69      Institutional                 Thriving              4   \n74      Institutional                Ascending              1   \n76      Institutional                 Thriving              3   \n84      Institutional                    Vital              1   \n93             Stable                 Emerging              1   \n94      Institutional                Ascending              1   \n105     Institutional                Ascending              1   \n111     Institutional                Ascending              1   \n117     Institutional                    Vital              1   \n118     Institutional                    Vital              2   \n130     Institutional                    Vital              1   \n131     Institutional                Ascending              1   \n133     Institutional                Ascending              1   \n135     Institutional                    Vital              1   \n137     Institutional                    Vital              2   \n143     Institutional                 Thriving              4   \n147     Institutional                 Emerging              0   \n158     Institutional                Ascending              1   \n161     Institutional                Ascending              0   \n162     Institutional                 Thriving              5   \n168     Institutional                    Vital              2   \n170     Institutional                    Vital              3   \n174     Institutional                    Vital              1   \n180     Institutional                Ascending              2   \n181     Institutional                Ascending              1   \n183            Stable                 Emerging              1   \n190     Institutional                    Vital              3   \n193     Institutional                 Thriving              4   \n195     Institutional                Ascending              2   \n196     Institutional                    Vital              2   \n198     Institutional                    Vital              2   \n200     Institutional                 Thriving              5   \n201     Institutional                 Thriving              5   \n203     Institutional                    Vital              2   \n\n     Bloom Train Data Percentage  XLM-R Train Tokens  \n7                        0.00007                 NaN  \n11                       4.60000              2869.0  \n16                       0.01000                 5.0  \n23                       0.00004                 NaN  \n27                       0.50000               525.0  \n35                       1.10000              1752.0  \n48                      30.04000             55608.0  \n51                       0.15000               270.0  \n56                       0.00020                 NaN  \n57                      12.90000              9780.0  \n65                       0.04000               140.0  \n69                       0.70000              1715.0  \n74                       0.00100                 NaN  \n76                       1.20000             22704.0  \n84                       0.06000               169.0  \n93                       0.00004                 NaN  \n94                       0.00300                 NaN  \n105                      0.00020                 NaN  \n111                      0.00040                 NaN  \n117                      0.10000               313.0  \n118                      0.05000               175.0  \n130                      0.07000               237.0  \n131                      0.00020                 NaN  \n133                      0.00010                 NaN  \n135                      0.04000                36.0  \n137                      0.05000                68.0  \n143                      4.90000              8405.0  \n147                      0.00030                 NaN  \n158                      0.00100                 NaN  \n161                      0.00007                 NaN  \n162                     10.80000              9374.0  \n168                      0.02000               275.0  \n170                      0.20000               595.0  \n174                      0.09000               249.0  \n180                      0.00020                 NaN  \n181                      0.00007                 NaN  \n183                      0.00002                 NaN  \n190                      0.10000               730.0  \n193                      2.70000             24757.0  \n195                      0.00040                 NaN  \n196                      0.00100                13.0  \n198                      0.00600                 NaN  \n200                     16.20000               259.0  \n201                      0.05000               176.0  \n203                      0.00100                 NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Folder Name</th>\n      <th>Language Name</th>\n      <th>ISO 639 - 1</th>\n      <th>ISO 639 - 2 (B)</th>\n      <th>ISO 639 - 2 (T)</th>\n      <th>ISO 639 - 3</th>\n      <th>Script (ISO 15924)</th>\n      <th>Language Family</th>\n      <th>Population</th>\n      <th>Language Vitality</th>\n      <th>Digital Language Support</th>\n      <th>Resource Level</th>\n      <th>Bloom Train Data Percentage</th>\n      <th>XLM-R Train Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>aka_Latn</td>\n      <td>Akan</td>\n      <td>ak</td>\n      <td>aka</td>\n      <td>aka</td>\n      <td>aka</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>arb_Arab</td>\n      <td>Standard Arabic</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>arb</td>\n      <td>Arab</td>\n      <td>Afro-Asiatic</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>4.60000</td>\n      <td>2869.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>asm_Beng</td>\n      <td>Assamese</td>\n      <td>as</td>\n      <td>asm</td>\n      <td>asm</td>\n      <td>asm</td>\n      <td>Beng</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.01000</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bam_Latn</td>\n      <td>Bamanankan</td>\n      <td>bm</td>\n      <td>bam</td>\n      <td>bam</td>\n      <td>bam</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00004</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ben_Beng</td>\n      <td>Bengali</td>\n      <td>bn</td>\n      <td>ben</td>\n      <td>ben</td>\n      <td>ben</td>\n      <td>Beng</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.50000</td>\n      <td>525.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>cat_Latn</td>\n      <td>Catalan</td>\n      <td>ca</td>\n      <td>cat</td>\n      <td>cat</td>\n      <td>cat</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>1.10000</td>\n      <td>1752.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>eng_Latn</td>\n      <td>English</td>\n      <td>en</td>\n      <td>eng</td>\n      <td>eng</td>\n      <td>eng</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>30.04000</td>\n      <td>55608.0</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>eus_Latn</td>\n      <td>Euskera</td>\n      <td>eu</td>\n      <td>baq</td>\n      <td>eus</td>\n      <td>eus</td>\n      <td>Latn</td>\n      <td>Isolate</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>4</td>\n      <td>0.15000</td>\n      <td>270.0</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>fon_Latn</td>\n      <td>Fon</td>\n      <td>NaN</td>\n      <td>fon</td>\n      <td>fon</td>\n      <td>fon</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Emerging</td>\n      <td>0</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>fra_Latn</td>\n      <td>French</td>\n      <td>fr</td>\n      <td>fre</td>\n      <td>fra</td>\n      <td>fra</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>12.90000</td>\n      <td>9780.0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>guj_Gujr</td>\n      <td>Gujarati</td>\n      <td>gu</td>\n      <td>guj</td>\n      <td>guj</td>\n      <td>guj</td>\n      <td>Gujr</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.04000</td>\n      <td>140.0</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>hin_Deva</td>\n      <td>Hindi</td>\n      <td>hi</td>\n      <td>hin</td>\n      <td>hin</td>\n      <td>hin</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>0.70000</td>\n      <td>1715.0</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>ibo_Latn</td>\n      <td>Igbo</td>\n      <td>ig</td>\n      <td>ibo</td>\n      <td>ibo</td>\n      <td>ibo</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>ind_Latn</td>\n      <td>Indonesian</td>\n      <td>id</td>\n      <td>ind</td>\n      <td>ind</td>\n      <td>ind</td>\n      <td>Latn</td>\n      <td>Austronesian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>3</td>\n      <td>1.20000</td>\n      <td>22704.0</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>kan_Knda</td>\n      <td>Kannada</td>\n      <td>kn</td>\n      <td>kan</td>\n      <td>kan</td>\n      <td>kan</td>\n      <td>Knda</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.06000</td>\n      <td>169.0</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>kik_Latn</td>\n      <td>Gikuyu</td>\n      <td>ki</td>\n      <td>kik</td>\n      <td>kik</td>\n      <td>kik</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Stable</td>\n      <td>Emerging</td>\n      <td>1</td>\n      <td>0.00004</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>kin_Latn</td>\n      <td>Kinyarwanda</td>\n      <td>rw</td>\n      <td>kin</td>\n      <td>kin</td>\n      <td>kin</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>lin_Latn</td>\n      <td>Lingala</td>\n      <td>ln</td>\n      <td>lin</td>\n      <td>lin</td>\n      <td>lin</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>lug_Latn</td>\n      <td>Ganda</td>\n      <td>lg</td>\n      <td>lug</td>\n      <td>lug</td>\n      <td>lug</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>mal_Mlym</td>\n      <td>Malayalam</td>\n      <td>ml</td>\n      <td>mal</td>\n      <td>mal</td>\n      <td>mal</td>\n      <td>Mlym</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.10000</td>\n      <td>313.0</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>mar_Deva</td>\n      <td>Marathi</td>\n      <td>mr</td>\n      <td>mar</td>\n      <td>mar</td>\n      <td>mar</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.05000</td>\n      <td>175.0</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>npi_Deva</td>\n      <td>Nepali</td>\n      <td>ne</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>npi</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.07000</td>\n      <td>237.0</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>nso_Latn</td>\n      <td>Northern Sotho</td>\n      <td>NaN</td>\n      <td>nso</td>\n      <td>nso</td>\n      <td>nso</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>nya_Latn</td>\n      <td>Chichewa</td>\n      <td>ny</td>\n      <td>nya</td>\n      <td>nya</td>\n      <td>nya</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00010</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>ory_Orya</td>\n      <td>Odia</td>\n      <td>or</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ory</td>\n      <td>Orya</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.04000</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>pan_Guru</td>\n      <td>Eastern Punjabi</td>\n      <td>pa</td>\n      <td>pan</td>\n      <td>pan</td>\n      <td>pan</td>\n      <td>Guru</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.05000</td>\n      <td>68.0</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>por_Latn</td>\n      <td>Portuguese</td>\n      <td>pt</td>\n      <td>por</td>\n      <td>por</td>\n      <td>por</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>4.90000</td>\n      <td>8405.0</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>run_Latn</td>\n      <td>Rundi</td>\n      <td>rn</td>\n      <td>run</td>\n      <td>run</td>\n      <td>run</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Emerging</td>\n      <td>0</td>\n      <td>0.00030</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>sna_Latn</td>\n      <td>Shona</td>\n      <td>sn</td>\n      <td>sna</td>\n      <td>sna</td>\n      <td>sna</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>sot_Latn</td>\n      <td>Southern Sotho</td>\n      <td>st</td>\n      <td>sot</td>\n      <td>sot</td>\n      <td>sot</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>0</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>spa_Latn</td>\n      <td>Spanish</td>\n      <td>es</td>\n      <td>spa</td>\n      <td>spa</td>\n      <td>spa</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>10.80000</td>\n      <td>9374.0</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>swh_Latn</td>\n      <td>Swahili</td>\n      <td>sw</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>swh</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.02000</td>\n      <td>275.0</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>tam_Taml</td>\n      <td>Tamil</td>\n      <td>ta</td>\n      <td>tam</td>\n      <td>tam</td>\n      <td>tam</td>\n      <td>Taml</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.20000</td>\n      <td>595.0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>tel_Telu</td>\n      <td>Telugu</td>\n      <td>te</td>\n      <td>tel</td>\n      <td>tel</td>\n      <td>tel</td>\n      <td>Telu</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.09000</td>\n      <td>249.0</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>tsn_Latn</td>\n      <td>Setswana</td>\n      <td>tn</td>\n      <td>tsn</td>\n      <td>tsn</td>\n      <td>tsn</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>2</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>tso_Latn</td>\n      <td>Tsonga</td>\n      <td>ts</td>\n      <td>tso</td>\n      <td>tso</td>\n      <td>tso</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>tum_Latn</td>\n      <td>Tumbuka</td>\n      <td>NaN</td>\n      <td>tum</td>\n      <td>tum</td>\n      <td>tum</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Stable</td>\n      <td>Emerging</td>\n      <td>1</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>urd_Arab</td>\n      <td>Urdu</td>\n      <td>ur</td>\n      <td>urd</td>\n      <td>urd</td>\n      <td>urd</td>\n      <td>Arab</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.10000</td>\n      <td>730.0</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>vie_Latn</td>\n      <td>Vietnamese</td>\n      <td>vi</td>\n      <td>vie</td>\n      <td>vie</td>\n      <td>vie</td>\n      <td>Latn</td>\n      <td>Austro-Asiatic</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>2.70000</td>\n      <td>24757.0</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>wol_Latn</td>\n      <td>Wolof</td>\n      <td>wo</td>\n      <td>wol</td>\n      <td>wol</td>\n      <td>wol</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>2</td>\n      <td>0.00040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>xho_Latn</td>\n      <td>Xhosa</td>\n      <td>xh</td>\n      <td>xho</td>\n      <td>xho</td>\n      <td>xho</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00100</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>yor_Latn</td>\n      <td>Yoruba</td>\n      <td>yo</td>\n      <td>yor</td>\n      <td>yor</td>\n      <td>yor</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00600</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>zho_Hans</td>\n      <td>Chinese</td>\n      <td>zh</td>\n      <td>chi</td>\n      <td>zho</td>\n      <td>zho</td>\n      <td>Hans</td>\n      <td>Sino-Tibetan</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>16.20000</td>\n      <td>259.0</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>zho_Hant</td>\n      <td>Chinese</td>\n      <td>zh</td>\n      <td>chi</td>\n      <td>zho</td>\n      <td>zho</td>\n      <td>Hant</td>\n      <td>Sino-Tibetan</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>0.05000</td>\n      <td>176.0</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>zul_Latn</td>\n      <td>Zulu</td>\n      <td>zu</td>\n      <td>zul</td>\n      <td>zul</td>\n      <td>zul</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('SIB-200 languages - ACL.xlsx')\n",
    "filtered_languages = df[df['Bloom Train Data Percentage'] > 0]\n",
    "filtered_languages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T09:19:09.850791346Z",
     "start_time": "2023-12-19T09:19:09.471937728Z"
    }
   },
   "id": "5b26c62f8fb04a28"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aka_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/19/2023 02:15:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False,\n",
      "Downloading config.json: 100%|██████████| 693/693 [00:00<00:00, 6.07MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 222/222 [00:00<00:00, 1.49MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 14.5M/14.5M [00:00<00:00, 29.5MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 172kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 2.13G/2.13G [00:51<00:00, 41.7MB/s]\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-1b1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='sib-200/data/annotated/aka_Latn', model_type='bloom', model_name_or_path='bigscience/bloom-1b1', output_dir='output_sib200/bigscience/bloom-1b1/aka_Latn_bert', output_prediction_file='test_predictions_aka_Latn', output_result='test_result_aka_Latn', labels='sib-200/data/annotated/aka_Latn/labels.txt', config_name='', tokenizer_name='', cache_dir='', max_seq_length=164, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, gradient_accumulation_steps=2, learning_rate=1e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=500000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   Creating features from dataset file at sib-200/data/annotated/aka_Latn\n",
      "/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Ɛpo atwa Turkey ho ahyia wɔ baabi mmiԑnsa: ‘Aegean Sea no wɔ atɔeԑ, Black Sea no wɔ atifi ԑnna Mediterranean Sea no wɔ anaafoɔ.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['Æ', 'Ĳ', 'po', 'Ġat', 'wa', 'ĠTurkey', 'Ġho', 'Ġah', 'y', 'ia', 'ĠwÉĶ', 'Ġba', 'abi', 'Ġm', 'mi', 'Ô', 'ĳ', 'n', 'sa', ':', 'ĠâĢĺ', 'A', 'eg', 'ean', 'ĠSea', 'Ġno', 'ĠwÉĶ', 'Ġat', 'ÉĶ', 'e', 'Ô', 'ĳ', ',', 'ĠBlack', 'ĠSea', 'Ġno', 'ĠwÉĶ', 'Ġat', 'ifi', 'ĠÔ', 'ĳ', 'n', 'na', 'ĠMediterranean', 'ĠSea', 'Ġno', 'ĠwÉĶ', 'Ġan', 'aa', 'fo', 'ÉĶ', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 132, 228, 3386, 919, 944, 67901, 1877, 4819, 92, 387, 64089, 2431, 20409, 279, 6209, 146, 229, 81, 2354, 29, 2648, 36, 1047, 2880, 47535, 654, 64089, 919, 3536, 72, 146, 229, 15, 21107, 47535, 654, 64089, 919, 12611, 81014, 229, 81, 2353, 110201, 47535, 654, 64089, 660, 15096, 2907, 3536, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Ɔko no ahyԑaseԑ no na wɔtaa fa asuo no ani, nanso berԑ rekɔ n’anim no ‘submarines’ no kɔɔ asuo no ase sԑdeԑ ԑbԑyԑ a wɔnhu wɔn.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['Æ', 'Ĩ', 'ko', 'Ġno', 'Ġah', 'y', 'Ô', 'ĳ', 'ase', 'Ô', 'ĳ', 'Ġno', 'Ġna', 'ĠwÉĶ', 'taa', 'Ġfa', 'Ġas', 'uo', 'Ġno', 'Ġan', 'i', ',', 'Ġn', 'anso', 'Ġber', 'Ô', 'ĳ', 'Ġrek', 'ÉĶ', 'ĠnâĢĻ', 'anim', 'Ġno', 'ĠâĢĺ', 'sub', 'mar', 'ines', 'âĢĻ', 'Ġno', 'ĠkÉĶ', 'ÉĶ', 'Ġas', 'uo', 'Ġno', 'Ġase', 'Ġs', 'Ô', 'ĳ', 'de', 'Ô', 'ĳ', 'ĠÔ', 'ĳ', 'b', 'Ô', 'ĳ', 'y', 'Ô', 'ĳ', 'Ġa', 'ĠwÉĶ', 'nh', 'u', 'Ġw', 'ÉĶn', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 132, 218, 1454, 654, 4819, 92, 146, 229, 1310, 146, 229, 654, 1109, 64089, 162175, 3237, 661, 22660, 654, 660, 76, 15, 294, 153171, 1099, 146, 229, 23075, 3536, 5394, 19572, 654, 2648, 7754, 7441, 3869, 726, 654, 119584, 3536, 661, 22660, 654, 52728, 272, 146, 229, 705, 146, 229, 81014, 229, 69, 146, 229, 92, 146, 229, 267, 64089, 667, 88, 372, 27402, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Berɛ a Hela kasa no reyera nkakrankakra no, Aman ahodoɔ a ɛwɔ Atɔeɛ fam no werɛ fii wɔn Hela nyansa ne wɔn nyansahunufoɔ nkyerɛkyerɛ no.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['Ber', 'ÉĽ', 'Ġa', 'ĠH', 'ela', 'Ġk', 'asa', 'Ġno', 'Ġrey', 'era', 'Ġnk', 'ak', 'rank', 'ak', 'ra', 'Ġno', ',', 'ĠAman', 'Ġah', 'odo', 'ÉĶ', 'Ġa', 'ĠÉĽwÉĶ', 'ĠAt', 'ÉĶ', 'e', 'ÉĽ', 'Ġfam', 'Ġno', 'Ġwer', 'ÉĽ', 'Ġfi', 'i', 'Ġw', 'ÉĶn', 'ĠH', 'ela', 'Ġny', 'ansa', 'Ġne', 'Ġw', 'ÉĶn', 'Ġny', 'ans', 'ahun', 'u', 'fo', 'ÉĶ', 'Ġn', 'ky', 'er', 'ÉĽ', 'ky', 'er', 'ÉĽ', 'Ġno', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 24038, 3755, 267, 620, 1814, 352, 2621, 654, 30693, 818, 26094, 395, 116290, 395, 702, 654, 15, 76753, 4819, 6660, 3536, 267, 241736, 5677, 3536, 72, 3755, 3110, 654, 30660, 3755, 7534, 76, 372, 27402, 620, 1814, 9329, 26692, 795, 372, 27402, 9329, 703, 13122, 88, 2907, 3536, 294, 11738, 259, 3755, 11738, 259, 3755, 654, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   Saving features into cached file sib-200/data/annotated/aka_Latn/cached_train_bloom-1b1_164\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   Creating features from dataset file at sib-200/data/annotated/aka_Latn\n",
      "/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Esiane sɛ wɔasesa mmirikatuo no firi anan mu nkyekyemu baako kɔ ɛfa no nti, ama ahohyɛsoɔ hu abehia sene ahoɔhare.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['Es', 'iane', 'ĠsÉĽ', 'ĠwÉĶ', 'as', 'esa', 'Ġmm', 'irik', 'atu', 'o', 'Ġno', 'Ġf', 'iri', 'Ġan', 'an', 'Ġmu', 'Ġn', 'ky', 'ek', 'y', 'emu', 'Ġba', 'ako', 'ĠkÉĶ', 'ĠÉĽ', 'fa', 'Ġno', 'Ġnti', ',', 'Ġama', 'Ġaho', 'hy', 'ÉĽ', 'so', 'ÉĶ', 'Ġhu', 'Ġab', 'eh', 'ia', 'Ġs', 'ene', 'Ġaho', 'ÉĶ', 'hare', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3788, 58507, 147395, 64089, 296, 6044, 18494, 16871, 1522, 82, 654, 319, 3292, 660, 256, 1499, 294, 11738, 902, 92, 12627, 2431, 1783, 119584, 63423, 4114, 654, 107580, 15, 32794, 79004, 7811, 3755, 1515, 3536, 7446, 1101, 4910, 387, 272, 3479, 79004, 3536, 241151, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Hwԑ akwantuo a ɔrebɔ ho dawuro no, wɔ wԑbsate so anaa wɔ ‘shop window’ so.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['H', 'w', 'Ô', 'ĳ', 'Ġak', 'w', 'antu', 'o', 'Ġa', 'ĠÉĶ', 're', 'bÉĶ', 'Ġho', 'Ġda', 'w', 'uro', 'Ġno', ',', 'ĠwÉĶ', 'Ġw', 'Ô', 'ĳ', 'bs', 'ate', 'Ġso', 'Ġan', 'aa', 'ĠwÉĶ', 'ĠâĢĺ', 'shop', 'Ġwindow', 'âĢĻ', 'Ġso', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 43, 90, 146, 229, 3088, 90, 10605, 82, 267, 35496, 289, 79898, 1877, 891, 90, 4413, 654, 15, 64089, 372, 146, 229, 25312, 655, 1427, 660, 15096, 64089, 2648, 42536, 17404, 726, 1427, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -   Tokenization example\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     text: Vatican City mu nnipa dodoɔ yɛ ahannwɔtwe. Ɛyɛ ɔman ketewa a adi ne ho wɔ wiase yi mu ne ɔman a emu nnipa sua.\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     tokens (by input): ['V', 'atic', 'an', 'ĠCity', 'Ġmu', 'Ġnn', 'ipa', 'Ġd', 'odo', 'ÉĶ', 'ĠyÉĽ', 'Ġah', 'ann', 'wÉĶ', 't', 'we', '.', 'ĠÆ', 'Ĳ', 'yÉĽ', 'ĠÉĶ', 'man', 'Ġk', 'ete', 'wa', 'Ġa', 'Ġadi', 'Ġne', 'Ġho', 'ĠwÉĶ', 'Ġwi', 'ase', 'Ġyi', 'Ġmu', 'Ġne', 'ĠÉĶ', 'man', 'Ġa', 'Ġem', 'u', 'Ġnn', 'ipa', 'Ġsua', '.']\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     token_ids: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 57, 3396, 256, 14585, 1499, 74577, 24571, 252, 6660, 3536, 155424, 4819, 3516, 79280, 87, 2136, 17, 137619, 228, 95327, 35496, 2111, 352, 5273, 944, 267, 63632, 795, 1877, 64089, 86024, 1310, 45184, 1499, 795, 35496, 2111, 267, 766, 88, 74577, 24571, 4711, 17]\n",
      "12/19/2023 02:16:53 - INFO - util_textclass -     attention mask: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   Saving features into cached file sib-200/data/annotated/aka_Latn/cached_dev_bloom-1b1_164\n",
      "/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "12/19/2023 02:16:53 - INFO - __main__ -   ***** Running training *****\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Num examples = 701\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Num Epochs = 10\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Gradient Accumulation steps = 2\n",
      "12/19/2023 02:16:53 - INFO - __main__ -     Total optimization steps = 220\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/44 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   2%|▏         | 1/44 [00:01<01:13,  1.71s/it]\u001B[A\n",
      "Epoch:   0%|          | 0/10 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 656, in <module>\n",
      "    main()\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 579, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, dev_dataset, labels, model, tokenizer)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 183, in train\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 1045, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "                          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 786, in forward\n",
      "    outputs = block(\n",
      "              ^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 463, in forward\n",
      "    output = self.mlp(layernorm_output, residual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 384, in forward\n",
      "    hidden_states = self.gelu_impl(self.dense_h_to_4h(hidden_states))\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 15.98 GiB of which 44.00 MiB is free. Of the allocated memory 14.75 GiB is allocated by PyTorch, and 836.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arb_Arab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/19/2023 02:16:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False,\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 656, in <module>\n",
      "    main()\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 561, in main\n",
      "    model = model_class.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 467, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 2611, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 1001, in __init__\n",
      "    self.transformer = BloomModel(config)\n",
      "                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 634, in __init__\n",
      "    self.h = nn.ModuleList([BloomBlock(config) for _ in range(config.num_hidden_layers)])\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 634, in <listcomp>\n",
      "    self.h = nn.ModuleList([BloomBlock(config) for _ in range(config.num_hidden_layers)])\n",
      "                            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 412, in __init__\n",
      "    self.mlp = BloomMLP(config)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 378, in __init__\n",
      "    self.dense_h_to_4h = nn.Linear(hidden_size, 4 * hidden_size)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 101, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 107, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/init.py\", line 419, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m SRC_DATA_DIR \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dir_name, SRC_LANG)\n\u001B[1;32m     25\u001B[0m OUTPUT_DIR \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_directory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMODEL\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSRC_LANG\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_bert\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 27\u001B[0m subprocess\u001B[38;5;241m.\u001B[39mrun([\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msib-200/code/train_textclass.py\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--data_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, SRC_DATA_DIR,\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--model_type\u001B[39m\u001B[38;5;124m\"\u001B[39m, MODEL_TYPE,\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m, MODEL,\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--output_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, OUTPUT_DIR,\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--output_result\u001B[39m\u001B[38;5;124m\"\u001B[39m, OUTPUT_FILE,\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--output_prediction_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, OUTPUT_PREDICTION,\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--max_seq_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, MAX_LENGTH,\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--num_train_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m, NUM_EPOCHS,\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--learning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1e-5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--per_gpu_train_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, BATCH_SIZE,\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--per_gpu_eval_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m, BATCH_SIZE,\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--save_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m, SAVE_STEPS,\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--seed\u001B[39m\u001B[38;5;124m\"\u001B[39m, SEED,\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--gradient_accumulation_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--labels\u001B[39m\u001B[38;5;124m\"\u001B[39m, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(SRC_DATA_DIR, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels.txt\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--do_train\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--do_eval\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--do_predict\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--overwrite_output_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     48\u001B[0m ])\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Remove unnecessary files\u001B[39;00m\n\u001B[1;32m     51\u001B[0m files_to_remove \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch_model.bin\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece.bpe.model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece.model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     60\u001B[0m ]\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/subprocess.py:550\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[1;32m    548\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[1;32m    549\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 550\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mcommunicate(\u001B[38;5;28minput\u001B[39m, timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TimeoutExpired \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    552\u001B[0m         process\u001B[38;5;241m.\u001B[39mkill()\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/subprocess.py:1201\u001B[0m, in \u001B[0;36mPopen.communicate\u001B[0;34m(self, input, timeout)\u001B[0m\n\u001B[1;32m   1199\u001B[0m         stderr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1200\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwait()\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1203\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/subprocess.py:1264\u001B[0m, in \u001B[0;36mPopen.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1262\u001B[0m     endtime \u001B[38;5;241m=\u001B[39m _time() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m   1263\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1266\u001B[0m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[1;32m   1267\u001B[0m     \u001B[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001B[39;00m\n\u001B[1;32m   1268\u001B[0m     \u001B[38;5;66;03m# exit under the common assumption that it also received the ^C\u001B[39;00m\n\u001B[1;32m   1269\u001B[0m     \u001B[38;5;66;03m# generated SIGINT and will exit rapidly.\u001B[39;00m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/subprocess.py:2046\u001B[0m, in \u001B[0;36mPopen._wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   2044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2045\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m  \u001B[38;5;66;03m# Another thread waited.\u001B[39;00m\n\u001B[0;32m-> 2046\u001B[0m (pid, sts) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_wait(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2047\u001B[0m \u001B[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001B[39;00m\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001B[39;00m\n\u001B[1;32m   2049\u001B[0m \u001B[38;5;66;03m# http://bugs.python.org/issue14396.\u001B[39;00m\n\u001B[1;32m   2050\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pid \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpid:\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/subprocess.py:2004\u001B[0m, in \u001B[0;36mPopen._try_wait\u001B[0;34m(self, wait_flags)\u001B[0m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2004\u001B[0m     (pid, sts) \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mwaitpid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpid, wait_flags)\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mChildProcessError\u001B[39;00m:\n\u001B[1;32m   2006\u001B[0m     \u001B[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001B[39;00m\n\u001B[1;32m   2007\u001B[0m     \u001B[38;5;66;03m# for child processes has otherwise been disabled for our\u001B[39;00m\n\u001B[1;32m   2008\u001B[0m     \u001B[38;5;66;03m# process.  This child is dead, we can't get the status.\u001B[39;00m\n\u001B[1;32m   2009\u001B[0m     pid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpid\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set environment variables\n",
    "output_directory = \"output_sib200\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "MAX_LENGTH = \"164\"\n",
    "BATCH_SIZE = \"16\"\n",
    "NUM_EPOCHS = \"10\"\n",
    "SAVE_STEPS = \"500000\"\n",
    "MODEL = \"bigscience/bloom-1b1\"\n",
    "MODEL_TYPE = \"bloom\"\n",
    "dir_name = \"sib-200/data/annotated\"\n",
    "SEED = \"42\"\n",
    "\n",
    "# Iterate over directories in dir_name\n",
    "for SRC_LANG_DIR in filtered_languages['Folder Name']:\n",
    "    # Extract SRC_LANG from directory name\n",
    "    SRC_LANG = os.path.basename(SRC_LANG_DIR)\n",
    "    print(SRC_LANG)\n",
    "\n",
    "    OUTPUT_FILE = f\"test_result_{SRC_LANG}\"\n",
    "    OUTPUT_PREDICTION = f\"test_predictions_{SRC_LANG}\"\n",
    "    SRC_DATA_DIR = os.path.join(dir_name, SRC_LANG)\n",
    "    OUTPUT_DIR = f\"{output_directory}/{MODEL}/{SRC_LANG}_bert\"\n",
    "\n",
    "    subprocess.run([\n",
    "        'python', 'sib-200/code/train_textclass.py',\n",
    "        \"--data_dir\", SRC_DATA_DIR,\n",
    "        \"--model_type\", MODEL_TYPE,\n",
    "        \"--model_name_or_path\", MODEL,\n",
    "        \"--output_dir\", OUTPUT_DIR,\n",
    "        \"--output_result\", OUTPUT_FILE,\n",
    "        \"--output_prediction_file\", OUTPUT_PREDICTION,\n",
    "        \"--max_seq_length\", MAX_LENGTH,\n",
    "        \"--num_train_epochs\", NUM_EPOCHS,\n",
    "        \"--learning_rate\", \"1e-5\",\n",
    "        \"--per_gpu_train_batch_size\", BATCH_SIZE,\n",
    "        \"--per_gpu_eval_batch_size\", BATCH_SIZE,\n",
    "        \"--save_steps\", SAVE_STEPS,\n",
    "        \"--seed\", SEED,\n",
    "        \"--gradient_accumulation_steps\", \"2\",\n",
    "        \"--labels\", os.path.join(SRC_DATA_DIR, 'labels.txt'),\n",
    "        \"--do_train\",\n",
    "        \"--do_eval\",\n",
    "        \"--do_predict\",\n",
    "        \"--overwrite_output_dir\"\n",
    "    ])\n",
    "\n",
    "    # Remove unnecessary files\n",
    "    files_to_remove = [\n",
    "        \"pytorch_model.bin\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"config.json\",\n",
    "        \"training_args.bin\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"sentencepiece.model\",\n",
    "    ]\n",
    "\n",
    "    for file_name in files_to_remove:\n",
    "        file_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T10:17:02.610557599Z",
     "start_time": "2023-12-19T10:15:50.238073171Z"
    }
   },
   "id": "61617cda088564ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9eafc32967241941"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
