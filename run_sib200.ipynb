{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "    Folder Name    Language Name ISO 639 - 1 ISO 639 - 2 (B) ISO 639 - 2 (T)  \\\n7      aka_Latn             Akan          ak             aka             aka   \n11     arb_Arab  Standard Arabic         NaN             NaN             NaN   \n16     asm_Beng         Assamese          as             asm             asm   \n23     bam_Latn       Bamanankan          bm             bam             bam   \n27     ben_Beng          Bengali          bn             ben             ben   \n35     cat_Latn          Catalan          ca             cat             cat   \n48     eng_Latn          English          en             eng             eng   \n51     eus_Latn          Euskera          eu             baq             eus   \n56     fon_Latn              Fon         NaN             fon             fon   \n57     fra_Latn           French          fr             fre             fra   \n65     guj_Gujr         Gujarati          gu             guj             guj   \n69     hin_Deva            Hindi          hi             hin             hin   \n74     ibo_Latn             Igbo          ig             ibo             ibo   \n76     ind_Latn       Indonesian          id             ind             ind   \n84     kan_Knda          Kannada          kn             kan             kan   \n93     kik_Latn           Gikuyu          ki             kik             kik   \n94     kin_Latn      Kinyarwanda          rw             kin             kin   \n105    lin_Latn          Lingala          ln             lin             lin   \n111    lug_Latn            Ganda          lg             lug             lug   \n117    mal_Mlym        Malayalam          ml             mal             mal   \n118    mar_Deva          Marathi          mr             mar             mar   \n130    npi_Deva           Nepali          ne             NaN             NaN   \n131    nso_Latn   Northern Sotho         NaN             nso             nso   \n133    nya_Latn         Chichewa          ny             nya             nya   \n135    ory_Orya             Odia          or             NaN             NaN   \n137    pan_Guru  Eastern Punjabi          pa             pan             pan   \n143    por_Latn       Portuguese          pt             por             por   \n147    run_Latn            Rundi          rn             run             run   \n158    sna_Latn            Shona          sn             sna             sna   \n161    sot_Latn   Southern Sotho          st             sot             sot   \n162    spa_Latn          Spanish          es             spa             spa   \n168    swh_Latn          Swahili          sw             NaN             NaN   \n170    tam_Taml            Tamil          ta             tam             tam   \n174    tel_Telu           Telugu          te             tel             tel   \n180    tsn_Latn         Setswana          tn             tsn             tsn   \n181    tso_Latn           Tsonga          ts             tso             tso   \n183    tum_Latn          Tumbuka         NaN             tum             tum   \n190    urd_Arab             Urdu          ur             urd             urd   \n193    vie_Latn       Vietnamese          vi             vie             vie   \n195    wol_Latn            Wolof          wo             wol             wol   \n196    xho_Latn            Xhosa          xh             xho             xho   \n198    yor_Latn           Yoruba          yo             yor             yor   \n200    zho_Hans          Chinese          zh             chi             zho   \n201    zho_Hant          Chinese          zh             chi             zho   \n203    zul_Latn             Zulu          zu             zul             zul   \n\n    ISO 639 - 3 Script (ISO 15924) Language Family              Population  \\\n7           aka               Latn     Niger-Congo  1 million to 1 billion   \n11          arb               Arab    Afro-Asiatic  1 million to 1 billion   \n16          asm               Beng   Indo-European  1 million to 1 billion   \n23          bam               Latn     Niger-Congo  1 million to 1 billion   \n27          ben               Beng   Indo-European  1 million to 1 billion   \n35          cat               Latn   Indo-European  1 million to 1 billion   \n48          eng               Latn   Indo-European          1 billion plus   \n51          eus               Latn         Isolate  1 million to 1 billion   \n56          fon               Latn     Niger-Congo  1 million to 1 billion   \n57          fra               Latn   Indo-European  1 million to 1 billion   \n65          guj               Gujr   Indo-European  1 million to 1 billion   \n69          hin               Deva   Indo-European  1 million to 1 billion   \n74          ibo               Latn     Niger-Congo  1 million to 1 billion   \n76          ind               Latn    Austronesian  1 million to 1 billion   \n84          kan               Knda       Dravidian  1 million to 1 billion   \n93          kik               Latn     Niger-Congo  1 million to 1 billion   \n94          kin               Latn     Niger-Congo  1 million to 1 billion   \n105         lin               Latn     Niger-Congo  1 million to 1 billion   \n111         lug               Latn     Niger-Congo  1 million to 1 billion   \n117         mal               Mlym       Dravidian  1 million to 1 billion   \n118         mar               Deva   Indo-European  1 million to 1 billion   \n130         npi               Deva   Indo-European  1 million to 1 billion   \n131         nso               Latn     Niger-Congo  1 million to 1 billion   \n133         nya               Latn     Niger-Congo  1 million to 1 billion   \n135         ory               Orya   Indo-European  1 million to 1 billion   \n137         pan               Guru   Indo-European  1 million to 1 billion   \n143         por               Latn   Indo-European  1 million to 1 billion   \n147         run               Latn     Niger-Congo  1 million to 1 billion   \n158         sna               Latn     Niger-Congo  1 million to 1 billion   \n161         sot               Latn     Niger-Congo  1 million to 1 billion   \n162         spa               Latn   Indo-European  1 million to 1 billion   \n168         swh               Latn     Niger-Congo  1 million to 1 billion   \n170         tam               Taml       Dravidian  1 million to 1 billion   \n174         tel               Telu       Dravidian  1 million to 1 billion   \n180         tsn               Latn     Niger-Congo  1 million to 1 billion   \n181         tso               Latn     Niger-Congo  1 million to 1 billion   \n183         tum               Latn     Niger-Congo  1 million to 1 billion   \n190         urd               Arab   Indo-European  1 million to 1 billion   \n193         vie               Latn  Austro-Asiatic  1 million to 1 billion   \n195         wol               Latn     Niger-Congo  1 million to 1 billion   \n196         xho               Latn     Niger-Congo  1 million to 1 billion   \n198         yor               Latn     Niger-Congo  1 million to 1 billion   \n200         zho               Hans    Sino-Tibetan          1 billion plus   \n201         zho               Hant    Sino-Tibetan          1 billion plus   \n203         zul               Latn     Niger-Congo  1 million to 1 billion   \n\n    Language Vitality Digital Language Support Resource Level  \\\n7       Institutional                Ascending              1   \n11      Institutional                 Thriving              5   \n16      Institutional                    Vital              1   \n23      Institutional                Ascending              1   \n27      Institutional                    Vital              3   \n35      Institutional                 Thriving              4   \n48      Institutional                 Thriving              5   \n51      Institutional                    Vital              4   \n56      Institutional                 Emerging              0   \n57      Institutional                 Thriving              5   \n65      Institutional                    Vital              1   \n69      Institutional                 Thriving              4   \n74      Institutional                Ascending              1   \n76      Institutional                 Thriving              3   \n84      Institutional                    Vital              1   \n93             Stable                 Emerging              1   \n94      Institutional                Ascending              1   \n105     Institutional                Ascending              1   \n111     Institutional                Ascending              1   \n117     Institutional                    Vital              1   \n118     Institutional                    Vital              2   \n130     Institutional                    Vital              1   \n131     Institutional                Ascending              1   \n133     Institutional                Ascending              1   \n135     Institutional                    Vital              1   \n137     Institutional                    Vital              2   \n143     Institutional                 Thriving              4   \n147     Institutional                 Emerging              0   \n158     Institutional                Ascending              1   \n161     Institutional                Ascending              0   \n162     Institutional                 Thriving              5   \n168     Institutional                    Vital              2   \n170     Institutional                    Vital              3   \n174     Institutional                    Vital              1   \n180     Institutional                Ascending              2   \n181     Institutional                Ascending              1   \n183            Stable                 Emerging              1   \n190     Institutional                    Vital              3   \n193     Institutional                 Thriving              4   \n195     Institutional                Ascending              2   \n196     Institutional                    Vital              2   \n198     Institutional                    Vital              2   \n200     Institutional                 Thriving              5   \n201     Institutional                 Thriving              5   \n203     Institutional                    Vital              2   \n\n     Bloom Train Data Percentage  XLM-R Train Tokens  \n7                        0.00007                 NaN  \n11                       4.60000              2869.0  \n16                       0.01000                 5.0  \n23                       0.00004                 NaN  \n27                       0.50000               525.0  \n35                       1.10000              1752.0  \n48                      30.04000             55608.0  \n51                       0.15000               270.0  \n56                       0.00020                 NaN  \n57                      12.90000              9780.0  \n65                       0.04000               140.0  \n69                       0.70000              1715.0  \n74                       0.00100                 NaN  \n76                       1.20000             22704.0  \n84                       0.06000               169.0  \n93                       0.00004                 NaN  \n94                       0.00300                 NaN  \n105                      0.00020                 NaN  \n111                      0.00040                 NaN  \n117                      0.10000               313.0  \n118                      0.05000               175.0  \n130                      0.07000               237.0  \n131                      0.00020                 NaN  \n133                      0.00010                 NaN  \n135                      0.04000                36.0  \n137                      0.05000                68.0  \n143                      4.90000              8405.0  \n147                      0.00030                 NaN  \n158                      0.00100                 NaN  \n161                      0.00007                 NaN  \n162                     10.80000              9374.0  \n168                      0.02000               275.0  \n170                      0.20000               595.0  \n174                      0.09000               249.0  \n180                      0.00020                 NaN  \n181                      0.00007                 NaN  \n183                      0.00002                 NaN  \n190                      0.10000               730.0  \n193                      2.70000             24757.0  \n195                      0.00040                 NaN  \n196                      0.00100                13.0  \n198                      0.00600                 NaN  \n200                     16.20000               259.0  \n201                      0.05000               176.0  \n203                      0.00100                 NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Folder Name</th>\n      <th>Language Name</th>\n      <th>ISO 639 - 1</th>\n      <th>ISO 639 - 2 (B)</th>\n      <th>ISO 639 - 2 (T)</th>\n      <th>ISO 639 - 3</th>\n      <th>Script (ISO 15924)</th>\n      <th>Language Family</th>\n      <th>Population</th>\n      <th>Language Vitality</th>\n      <th>Digital Language Support</th>\n      <th>Resource Level</th>\n      <th>Bloom Train Data Percentage</th>\n      <th>XLM-R Train Tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>aka_Latn</td>\n      <td>Akan</td>\n      <td>ak</td>\n      <td>aka</td>\n      <td>aka</td>\n      <td>aka</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>arb_Arab</td>\n      <td>Standard Arabic</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>arb</td>\n      <td>Arab</td>\n      <td>Afro-Asiatic</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>4.60000</td>\n      <td>2869.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>asm_Beng</td>\n      <td>Assamese</td>\n      <td>as</td>\n      <td>asm</td>\n      <td>asm</td>\n      <td>asm</td>\n      <td>Beng</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.01000</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bam_Latn</td>\n      <td>Bamanankan</td>\n      <td>bm</td>\n      <td>bam</td>\n      <td>bam</td>\n      <td>bam</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00004</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ben_Beng</td>\n      <td>Bengali</td>\n      <td>bn</td>\n      <td>ben</td>\n      <td>ben</td>\n      <td>ben</td>\n      <td>Beng</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.50000</td>\n      <td>525.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>cat_Latn</td>\n      <td>Catalan</td>\n      <td>ca</td>\n      <td>cat</td>\n      <td>cat</td>\n      <td>cat</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>1.10000</td>\n      <td>1752.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>eng_Latn</td>\n      <td>English</td>\n      <td>en</td>\n      <td>eng</td>\n      <td>eng</td>\n      <td>eng</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>30.04000</td>\n      <td>55608.0</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>eus_Latn</td>\n      <td>Euskera</td>\n      <td>eu</td>\n      <td>baq</td>\n      <td>eus</td>\n      <td>eus</td>\n      <td>Latn</td>\n      <td>Isolate</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>4</td>\n      <td>0.15000</td>\n      <td>270.0</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>fon_Latn</td>\n      <td>Fon</td>\n      <td>NaN</td>\n      <td>fon</td>\n      <td>fon</td>\n      <td>fon</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Emerging</td>\n      <td>0</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>fra_Latn</td>\n      <td>French</td>\n      <td>fr</td>\n      <td>fre</td>\n      <td>fra</td>\n      <td>fra</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>12.90000</td>\n      <td>9780.0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>guj_Gujr</td>\n      <td>Gujarati</td>\n      <td>gu</td>\n      <td>guj</td>\n      <td>guj</td>\n      <td>guj</td>\n      <td>Gujr</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.04000</td>\n      <td>140.0</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>hin_Deva</td>\n      <td>Hindi</td>\n      <td>hi</td>\n      <td>hin</td>\n      <td>hin</td>\n      <td>hin</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>0.70000</td>\n      <td>1715.0</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>ibo_Latn</td>\n      <td>Igbo</td>\n      <td>ig</td>\n      <td>ibo</td>\n      <td>ibo</td>\n      <td>ibo</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>ind_Latn</td>\n      <td>Indonesian</td>\n      <td>id</td>\n      <td>ind</td>\n      <td>ind</td>\n      <td>ind</td>\n      <td>Latn</td>\n      <td>Austronesian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>3</td>\n      <td>1.20000</td>\n      <td>22704.0</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>kan_Knda</td>\n      <td>Kannada</td>\n      <td>kn</td>\n      <td>kan</td>\n      <td>kan</td>\n      <td>kan</td>\n      <td>Knda</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.06000</td>\n      <td>169.0</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>kik_Latn</td>\n      <td>Gikuyu</td>\n      <td>ki</td>\n      <td>kik</td>\n      <td>kik</td>\n      <td>kik</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Stable</td>\n      <td>Emerging</td>\n      <td>1</td>\n      <td>0.00004</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>kin_Latn</td>\n      <td>Kinyarwanda</td>\n      <td>rw</td>\n      <td>kin</td>\n      <td>kin</td>\n      <td>kin</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>lin_Latn</td>\n      <td>Lingala</td>\n      <td>ln</td>\n      <td>lin</td>\n      <td>lin</td>\n      <td>lin</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>lug_Latn</td>\n      <td>Ganda</td>\n      <td>lg</td>\n      <td>lug</td>\n      <td>lug</td>\n      <td>lug</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>mal_Mlym</td>\n      <td>Malayalam</td>\n      <td>ml</td>\n      <td>mal</td>\n      <td>mal</td>\n      <td>mal</td>\n      <td>Mlym</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.10000</td>\n      <td>313.0</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>mar_Deva</td>\n      <td>Marathi</td>\n      <td>mr</td>\n      <td>mar</td>\n      <td>mar</td>\n      <td>mar</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.05000</td>\n      <td>175.0</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>npi_Deva</td>\n      <td>Nepali</td>\n      <td>ne</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>npi</td>\n      <td>Deva</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.07000</td>\n      <td>237.0</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>nso_Latn</td>\n      <td>Northern Sotho</td>\n      <td>NaN</td>\n      <td>nso</td>\n      <td>nso</td>\n      <td>nso</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>nya_Latn</td>\n      <td>Chichewa</td>\n      <td>ny</td>\n      <td>nya</td>\n      <td>nya</td>\n      <td>nya</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00010</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>ory_Orya</td>\n      <td>Odia</td>\n      <td>or</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ory</td>\n      <td>Orya</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.04000</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>pan_Guru</td>\n      <td>Eastern Punjabi</td>\n      <td>pa</td>\n      <td>pan</td>\n      <td>pan</td>\n      <td>pan</td>\n      <td>Guru</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.05000</td>\n      <td>68.0</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>por_Latn</td>\n      <td>Portuguese</td>\n      <td>pt</td>\n      <td>por</td>\n      <td>por</td>\n      <td>por</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>4.90000</td>\n      <td>8405.0</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>run_Latn</td>\n      <td>Rundi</td>\n      <td>rn</td>\n      <td>run</td>\n      <td>run</td>\n      <td>run</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Emerging</td>\n      <td>0</td>\n      <td>0.00030</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>sna_Latn</td>\n      <td>Shona</td>\n      <td>sn</td>\n      <td>sna</td>\n      <td>sna</td>\n      <td>sna</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>sot_Latn</td>\n      <td>Southern Sotho</td>\n      <td>st</td>\n      <td>sot</td>\n      <td>sot</td>\n      <td>sot</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>0</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>spa_Latn</td>\n      <td>Spanish</td>\n      <td>es</td>\n      <td>spa</td>\n      <td>spa</td>\n      <td>spa</td>\n      <td>Latn</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>10.80000</td>\n      <td>9374.0</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>swh_Latn</td>\n      <td>Swahili</td>\n      <td>sw</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>swh</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.02000</td>\n      <td>275.0</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>tam_Taml</td>\n      <td>Tamil</td>\n      <td>ta</td>\n      <td>tam</td>\n      <td>tam</td>\n      <td>tam</td>\n      <td>Taml</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.20000</td>\n      <td>595.0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>tel_Telu</td>\n      <td>Telugu</td>\n      <td>te</td>\n      <td>tel</td>\n      <td>tel</td>\n      <td>tel</td>\n      <td>Telu</td>\n      <td>Dravidian</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>1</td>\n      <td>0.09000</td>\n      <td>249.0</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>tsn_Latn</td>\n      <td>Setswana</td>\n      <td>tn</td>\n      <td>tsn</td>\n      <td>tsn</td>\n      <td>tsn</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>2</td>\n      <td>0.00020</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>tso_Latn</td>\n      <td>Tsonga</td>\n      <td>ts</td>\n      <td>tso</td>\n      <td>tso</td>\n      <td>tso</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>1</td>\n      <td>0.00007</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>tum_Latn</td>\n      <td>Tumbuka</td>\n      <td>NaN</td>\n      <td>tum</td>\n      <td>tum</td>\n      <td>tum</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Stable</td>\n      <td>Emerging</td>\n      <td>1</td>\n      <td>0.00002</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>urd_Arab</td>\n      <td>Urdu</td>\n      <td>ur</td>\n      <td>urd</td>\n      <td>urd</td>\n      <td>urd</td>\n      <td>Arab</td>\n      <td>Indo-European</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>3</td>\n      <td>0.10000</td>\n      <td>730.0</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>vie_Latn</td>\n      <td>Vietnamese</td>\n      <td>vi</td>\n      <td>vie</td>\n      <td>vie</td>\n      <td>vie</td>\n      <td>Latn</td>\n      <td>Austro-Asiatic</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>4</td>\n      <td>2.70000</td>\n      <td>24757.0</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>wol_Latn</td>\n      <td>Wolof</td>\n      <td>wo</td>\n      <td>wol</td>\n      <td>wol</td>\n      <td>wol</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Ascending</td>\n      <td>2</td>\n      <td>0.00040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>xho_Latn</td>\n      <td>Xhosa</td>\n      <td>xh</td>\n      <td>xho</td>\n      <td>xho</td>\n      <td>xho</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00100</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>yor_Latn</td>\n      <td>Yoruba</td>\n      <td>yo</td>\n      <td>yor</td>\n      <td>yor</td>\n      <td>yor</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00600</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>zho_Hans</td>\n      <td>Chinese</td>\n      <td>zh</td>\n      <td>chi</td>\n      <td>zho</td>\n      <td>zho</td>\n      <td>Hans</td>\n      <td>Sino-Tibetan</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>16.20000</td>\n      <td>259.0</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>zho_Hant</td>\n      <td>Chinese</td>\n      <td>zh</td>\n      <td>chi</td>\n      <td>zho</td>\n      <td>zho</td>\n      <td>Hant</td>\n      <td>Sino-Tibetan</td>\n      <td>1 billion plus</td>\n      <td>Institutional</td>\n      <td>Thriving</td>\n      <td>5</td>\n      <td>0.05000</td>\n      <td>176.0</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>zul_Latn</td>\n      <td>Zulu</td>\n      <td>zu</td>\n      <td>zul</td>\n      <td>zul</td>\n      <td>zul</td>\n      <td>Latn</td>\n      <td>Niger-Congo</td>\n      <td>1 million to 1 billion</td>\n      <td>Institutional</td>\n      <td>Vital</td>\n      <td>2</td>\n      <td>0.00100</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('SIB-200 languages - ACL.xlsx')\n",
    "filtered_languages = df[df['Bloom Train Data Percentage'] > 0]\n",
    "filtered_languages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T03:48:49.991614610Z",
     "start_time": "2024-01-05T03:48:49.617157466Z"
    }
   },
   "id": "5b26c62f8fb04a28"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aka_Latn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/04/2024 19:48:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False,\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-1b1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/04/2024 19:49:03 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='sib-200/data/annotated/aka_Latn', model_type='bloom', model_name_or_path='bigscience/bloom-1b1', output_dir='output_sib200/bigscience/bloom-1b1/aka_Latn_bert', output_prediction_file='test_predictions_aka_Latn', output_result='test_result_aka_Latn', labels='sib-200/data/annotated/aka_Latn/labels.txt', config_name='', tokenizer_name='', cache_dir='', max_seq_length=164, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, gradient_accumulation_steps=2, learning_rate=1e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=500000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
      "01/04/2024 19:49:03 - INFO - __main__ -   Loading features from cached file sib-200/data/annotated/aka_Latn/cached_train_bloom-1b1_164\n",
      "01/04/2024 19:49:03 - INFO - __main__ -   Loading features from cached file sib-200/data/annotated/aka_Latn/cached_dev_bloom-1b1_164\n",
      "/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "01/04/2024 19:49:03 - INFO - __main__ -   ***** Running training *****\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Num examples = 701\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Num Epochs = 10\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Gradient Accumulation steps = 2\n",
      "01/04/2024 19:49:03 - INFO - __main__ -     Total optimization steps = 220\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/44 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   2%|â–         | 1/44 [00:01<01:18,  1.82s/it]\u001B[A\n",
      "Epoch:   0%|          | 0/10 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 656, in <module>\n",
      "    main()\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 579, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, dev_dataset, labels, model, tokenizer)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/Codes/LLMSize/sib-200/code/train_textclass.py\", line 183, in train\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 1045, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "                          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 786, in forward\n",
      "    outputs = block(\n",
      "              ^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 463, in forward\n",
      "    output = self.mlp(layernorm_output, residual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/models/bloom/modeling_bloom.py\", line 384, in forward\n",
      "    hidden_states = self.gelu_impl(self.dense_h_to_4h(hidden_states))\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sina/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 15.98 GiB of which 44.00 MiB is free. Of the allocated memory 14.75 GiB is allocated by PyTorch, and 836.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set environment variables\n",
    "output_directory = \"output_sib200\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "MAX_LENGTH = \"164\"\n",
    "BATCH_SIZE = \"16\"\n",
    "NUM_EPOCHS = \"10\"\n",
    "SAVE_STEPS = \"500000\"\n",
    "MODEL = \"bigscience/bloom-1b1\"\n",
    "MODEL_TYPE = \"bloom\"\n",
    "dir_name = \"sib-200/data/annotated\"\n",
    "SEED = \"42\"\n",
    "\n",
    "# Iterate over directories in dir_name\n",
    "for SRC_LANG_DIR in filtered_languages['Folder Name'][:1]:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Extract SRC_LANG from directory name\n",
    "    SRC_LANG = os.path.basename(SRC_LANG_DIR)\n",
    "    print(SRC_LANG)\n",
    "\n",
    "    OUTPUT_FILE = f\"test_result_{SRC_LANG}\"\n",
    "    OUTPUT_PREDICTION = f\"test_predictions_{SRC_LANG}\"\n",
    "    SRC_DATA_DIR = os.path.join(dir_name, SRC_LANG)\n",
    "    OUTPUT_DIR = f\"{output_directory}/{MODEL}/{SRC_LANG}_bert\"\n",
    "\n",
    "    subprocess.run([\n",
    "        'python', 'sib-200/code/train_textclass.py',\n",
    "        \"--data_dir\", SRC_DATA_DIR,\n",
    "        \"--model_type\", MODEL_TYPE,\n",
    "        \"--model_name_or_path\", MODEL,\n",
    "        \"--output_dir\", OUTPUT_DIR,\n",
    "        \"--output_result\", OUTPUT_FILE,\n",
    "        \"--output_prediction_file\", OUTPUT_PREDICTION,\n",
    "        \"--max_seq_length\", MAX_LENGTH,\n",
    "        \"--num_train_epochs\", NUM_EPOCHS,\n",
    "        \"--learning_rate\", \"1e-5\",\n",
    "        \"--per_gpu_train_batch_size\", BATCH_SIZE,\n",
    "        \"--per_gpu_eval_batch_size\", BATCH_SIZE,\n",
    "        \"--save_steps\", SAVE_STEPS,\n",
    "        \"--seed\", SEED,\n",
    "        \"--gradient_accumulation_steps\", \"2\",\n",
    "        \"--labels\", os.path.join(SRC_DATA_DIR, 'labels.txt'),\n",
    "        \"--do_train\",\n",
    "        \"--do_eval\",\n",
    "        \"--do_predict\",\n",
    "        \"--overwrite_output_dir\"\n",
    "    ])\n",
    "\n",
    "    # Remove unnecessary files\n",
    "    files_to_remove = [\n",
    "        \"pytorch_model.bin\",\n",
    "        \"sentencepiece.bpe.model\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"config.json\",\n",
    "        \"training_args.bin\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"sentencepiece.model\",\n",
    "    ]\n",
    "\n",
    "    for file_name in files_to_remove:\n",
    "        file_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T03:49:06.773272112Z",
     "start_time": "2024-01-05T03:48:54.176771587Z"
    }
   },
   "id": "61617cda088564ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2176dafaa65f4531"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
