{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/shn_Mymr.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/lao_Laoo.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/taq_Tfng.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/mya_Mymr.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/khm_Khmr.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/dzo_Tibt.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/tzm_Tfng.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/sat_Olck.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/sin_Sinh.csv\n",
      "Results saved to zero-shot/sib-200/Bloomz-3b/beam-search/bod_Tibt.csv\n"
     ]
    }
   ],
   "source": [
    "## ZeroShot learning for SIB-200 dataset on Bloom model with beam search\n",
    "# 57, 58, 71, 86, 91, 92, 115, 129, 150, 164\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "# Load ZeroShot learning model and tokenizer\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# List of labels to use for ZeroShot learning\n",
    "list_of_labels = [\"science\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", \"geography\"]\n",
    "\n",
    "# Function to predict category given text\n",
    "def predict_category(text):\n",
    "    l = \"\\n- \".join(list_of_labels)\n",
    "    prompt = f\"SENTENCE:\\n {text} \\n Is this SENTENCE {', '.join(list_of_labels)}? \\nOPTIONS:\\n-{l}\\n-ANSWER:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    result_length = len(tokens) + 10\n",
    "    generated_text = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                                                     max_length=result_length,\n",
    "                                                     num_beams=4,\n",
    "                                                     no_repeat_ngram_size=2,\n",
    "                                                     early_stopping=True\n",
    "                                                     )[0])\n",
    "\n",
    "    del inputs\n",
    "\n",
    "    found_labels = 0\n",
    "    found_label = \"\"\n",
    "\n",
    "    for label in list_of_labels:\n",
    "        if label in generated_text[generated_text.find('ANSWER')+3:].lower():\n",
    "            found_labels += 1\n",
    "            found_label = label\n",
    "\n",
    "    if found_labels == 1:\n",
    "        return found_label, generated_text\n",
    "    else:\n",
    "        return \"N/A\", generated_text\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"zero-shot/sib-200/Bloomz-3b/beam-search\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through subdirectories in the data directory\n",
    "# for language in os.listdir(data_directory):\n",
    "for language in [os.listdir(data_directory)[i] for i in [57, 58, 71, 86, 91, 92, 115, 129, 150, 164]]:\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'actual_category', 'predicted_category', 'generated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, language)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Iterate through rows and compare predicted category with actual category\n",
    "            for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                actual_category = row['category']\n",
    "\n",
    "                # Predict category using your ZeroShot learning model\n",
    "                predicted_category, generated_text = predict_category(text)\n",
    "\n",
    "                # Append the results to the DataFrame\n",
    "                results_df = results_df.append({'text': text,\n",
    "                                                'actual_category': actual_category,\n",
    "                                                'predicted_category': 'science/technology' if predicted_category == 'science' else predicted_category,\n",
    "                                                'generated_text': generated_text}, ignore_index=True)\n",
    "                torch.cuda.empty_cache()\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{language}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T10:16:18.694329796Z",
     "start_time": "2024-01-19T03:47:15.658713971Z"
    }
   },
   "id": "89a087fa2a408ae6",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Calculte F1 score for each language and add it to the DataFrame\n",
    "def calculate_f1(language, model, method):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    results_df = pd.read_csv(f\"zero-shot/sib-200/{model}/{method}/{language}.csv\")\n",
    "\n",
    "    # Replace NaN values with 'N/A'\n",
    "    results_df = results_df.fillna('N/A')\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(results_df['actual_category'], results_df['predicted_category'], average='macro')\n",
    "\n",
    "    # Add the F1 score to the DataFrame\n",
    "    df.loc[df['Folder Name'] == language, f'F1 {model} {method}'] = f1\n",
    "\n",
    "# Iterate through languages and calculate F1 score for each language\n",
    "for language in df['Folder Name']:\n",
    "    calculate_f1(language, \"Bloomz-560M\", \"beam-search\")\n",
    "    calculate_f1(language, \"Bloomz-1b1\", \"beam-search\")\n",
    "    calculate_f1(language, \"Bloomz-1b7\", \"beam-search\")\n",
    "    calculate_f1(language, \"Bloomz-3b\", \"beam-search\")\n",
    "\n",
    "# Save the updated DataFrame to the Excel file\n",
    "df.to_excel(\"SIB-200 languages - ACL.xlsx\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T18:18:42.874015750Z",
     "start_time": "2024-01-19T18:18:38.366652851Z"
    }
   },
   "id": "7a2b195881c57d00",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to zero-shot/sib-200/Test/beam-search/eng_Latn.csv\n"
     ]
    }
   ],
   "source": [
    "#Tests\n",
    "## ZeroShot learning for SIB-200 dataset on Bloom-560M model with sampling\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "# Load ZeroShot learning model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# List of labels to use for ZeroShot learning\n",
    "list_of_labels = [\"science\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", \"geography\"]\n",
    "\n",
    "# Function to predict category given text\n",
    "def predict_category(text):\n",
    "    # prompt = f\"\\\"{text}\\\" What category does this sentence belong to? {', '.join(list_of_labels)}?? The correct answer is:\"\n",
    "    # prompt = f\"Classify each sentence into one of 7 classes: [{', '.join(list_of_labels)}] \\n Sentence: {text} \\n Class:\"\n",
    "    # prompt = f\"Here is a sentence: \\\"{text}\\\" This is list of categories: {', '.join(list_of_labels)}. \\n What category does this sentence belong to? Give me the correct category without extra text. \"\n",
    "    prompt = f\"SENTENCE:\\n {text} \\n Is this SENTENCE science, travel, politics, sports, health, entertainment or geography? \\nOPTIONS:\\n-science \\n-travel \\n-politics \\n-sports \\n-health \\n-entertainment \\n-geography \\n-ANSWER:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    result_length = len(tokens) + 10\n",
    "    generated_text = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                                                     max_length=result_length,\n",
    "                                                     num_beams=4,\n",
    "                                                     no_repeat_ngram_size=2,\n",
    "                                                     early_stopping=True\n",
    "                                                     )[0])\n",
    "\n",
    "    del inputs\n",
    "\n",
    "    found_labels = 0\n",
    "    found_label = \"\"\n",
    "\n",
    "    for label in list_of_labels:\n",
    "        if label in generated_text[generated_text.find('ANSWER')+3:].lower():\n",
    "            found_labels += 1\n",
    "            found_label = label\n",
    "\n",
    "    if found_labels == 1:\n",
    "        return found_label, generated_text\n",
    "    else:\n",
    "        return \"N/A\", generated_text\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"zero-shot/sib-200/Test/beam-search\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through subdirectories in the data directory\n",
    "# for language in os.listdir(data_directory):\n",
    "for language in ['eng_Latn']:\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'actual_category', 'predicted_category', 'generated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, language)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Iterate through rows and compare predicted category with actual category\n",
    "            for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                actual_category = row['category']\n",
    "\n",
    "                # Predict category using your ZeroShot learning model\n",
    "                predicted_category, generated_text = predict_category(text)\n",
    "\n",
    "                # Append the results to the DataFrame\n",
    "                results_df = results_df.append({'text': text,\n",
    "                                                'actual_category': actual_category,\n",
    "                                                'predicted_category': 'science/technology' if predicted_category == 'science' else predicted_category,\n",
    "                                                'generated_text': generated_text}, ignore_index=True)\n",
    "                torch.cuda.empty_cache()\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{language}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T23:53:14.296754328Z",
     "start_time": "2024-01-17T23:53:01.495956206Z"
    }
   },
   "id": "14607649042d5ebd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "982bf05ee00a4cc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
