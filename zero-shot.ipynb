{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 92\u001B[0m\n\u001B[1;32m     89\u001B[0m actual_category \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m# Predict category using your ZeroShot learning model\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m predicted_category, generated_text \u001B[38;5;241m=\u001B[39m predict_category(text)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Append the results to the DataFrame\u001B[39;00m\n\u001B[1;32m     95\u001B[0m results_df \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m: text,\n\u001B[1;32m     96\u001B[0m                                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactual_category\u001B[39m\u001B[38;5;124m'\u001B[39m: actual_category,\n\u001B[1;32m     97\u001B[0m                                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted_category\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscience/technology\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m predicted_category \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscience\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m predicted_category,\n\u001B[1;32m     98\u001B[0m                                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m: generated_text}, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[2], line 43\u001B[0m, in \u001B[0;36mpredict_category\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     41\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(prompt, add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m result_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokens) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m---> 43\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(model\u001B[38;5;241m.\u001B[39mgenerate(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     44\u001B[0m                                                  max_length\u001B[38;5;241m=\u001B[39mresult_length,\n\u001B[1;32m     45\u001B[0m                                                  num_beams\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     46\u001B[0m                                                  no_repeat_ngram_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m     47\u001B[0m                                                  early_stopping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     48\u001B[0m                                                  )[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m     52\u001B[0m found_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/utils.py:1604\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001B[0m\n\u001B[1;32m   1597\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   1598\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1599\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   1600\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   1601\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1602\u001B[0m     )\n\u001B[1;32m   1603\u001B[0m     \u001B[38;5;66;03m# 13. run beam search\u001B[39;00m\n\u001B[0;32m-> 1604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeam_search(\n\u001B[1;32m   1605\u001B[0m         input_ids,\n\u001B[1;32m   1606\u001B[0m         beam_scorer,\n\u001B[1;32m   1607\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1608\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1609\u001B[0m         pad_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mpad_token_id,\n\u001B[1;32m   1610\u001B[0m         eos_token_id\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[1;32m   1611\u001B[0m         output_scores\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39moutput_scores,\n\u001B[1;32m   1612\u001B[0m         return_dict_in_generate\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mreturn_dict_in_generate,\n\u001B[1;32m   1613\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[1;32m   1614\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1615\u001B[0m     )\n\u001B[1;32m   1617\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_beam_sample_gen_mode:\n\u001B[1;32m   1618\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[1;32m   1619\u001B[0m     logits_warper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config)\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/utils.py:2921\u001B[0m, in \u001B[0;36mGenerationMixin.beam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   2916\u001B[0m next_token_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madjust_logits_during_generation(next_token_logits, cur_len\u001B[38;5;241m=\u001B[39mcur_len)\n\u001B[1;32m   2917\u001B[0m next_token_scores \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlog_softmax(\n\u001B[1;32m   2918\u001B[0m     next_token_logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2919\u001B[0m )  \u001B[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001B[39;00m\n\u001B[0;32m-> 2921\u001B[0m next_token_scores_processed \u001B[38;5;241m=\u001B[39m logits_processor(input_ids, next_token_scores)\n\u001B[1;32m   2922\u001B[0m next_token_scores \u001B[38;5;241m=\u001B[39m next_token_scores_processed \u001B[38;5;241m+\u001B[39m beam_scores[:, \u001B[38;5;28;01mNone\u001B[39;00m]\u001B[38;5;241m.\u001B[39mexpand_as(next_token_scores)\n\u001B[1;32m   2924\u001B[0m \u001B[38;5;66;03m# Store scores, attentions and hidden_states when required\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/logits_process.py:92\u001B[0m, in \u001B[0;36mLogitsProcessorList.__call__\u001B[0;34m(self, input_ids, scores, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m         scores \u001B[38;5;241m=\u001B[39m processor(input_ids, scores, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 92\u001B[0m         scores \u001B[38;5;241m=\u001B[39m processor(input_ids, scores)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m scores\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/logits_process.py:492\u001B[0m, in \u001B[0;36mNoRepeatNGramLogitsProcessor.__call__\u001B[0;34m(self, input_ids, scores)\u001B[0m\n\u001B[1;32m    490\u001B[0m num_batch_hypotheses \u001B[38;5;241m=\u001B[39m scores\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    491\u001B[0m cur_len \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 492\u001B[0m banned_batch_tokens \u001B[38;5;241m=\u001B[39m _calc_banned_ngram_tokens(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, banned_tokens \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(banned_batch_tokens):\n\u001B[1;32m    495\u001B[0m     scores[i, banned_tokens] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/logits_process.py:465\u001B[0m, in \u001B[0;36m_calc_banned_ngram_tokens\u001B[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cur_len \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<\u001B[39m ngram_size:\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;66;03m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001B[39;00m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_hypos)]\n\u001B[0;32m--> 465\u001B[0m generated_ngrams \u001B[38;5;241m=\u001B[39m _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n\u001B[1;32m    467\u001B[0m banned_tokens \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    468\u001B[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hypo_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_hypos)\n\u001B[1;32m    470\u001B[0m ]\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m banned_tokens\n",
      "File \u001B[0;32m~/anaconda3/envs/sib200/lib/python3.11/site-packages/transformers/generation/logits_process.py:442\u001B[0m, in \u001B[0;36m_get_ngrams\u001B[0;34m(ngram_size, prev_input_ids, num_hypos)\u001B[0m\n\u001B[1;32m    440\u001B[0m generated_ngrams \u001B[38;5;241m=\u001B[39m [{} \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_hypos)]\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_hypos):\n\u001B[0;32m--> 442\u001B[0m     gen_tokens \u001B[38;5;241m=\u001B[39m prev_input_ids[idx]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    443\u001B[0m     generated_ngram \u001B[38;5;241m=\u001B[39m generated_ngrams[idx]\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ngram \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m[gen_tokens[i:] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(ngram_size)]):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## ZeroShot learning for SIB-200 dataset by generating text\n",
    "\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "\n",
    "\n",
    "model_address = \"facebook/xglm-564M\"\n",
    "\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "# # Load ZeroShot learning model and tokenizer\n",
    "# model = BloomForCausalLM.from_pretrained(model_address)\n",
    "# tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# List of labels to use for ZeroShot learning\n",
    "list_of_labels = [\"science\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", \"geography\"]\n",
    "\n",
    "# Function to predict category given text\n",
    "def predict_category(text):\n",
    "    l = \"\\n- \".join(list_of_labels)\n",
    "    prompt = f\"SENTENCE:\\n {text} \\n Is this SENTENCE {', '.join(list_of_labels)}? \\nOPTIONS:\\n-{l}\\n-ANSWER: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    result_length = len(tokens) + 10\n",
    "    generated_text = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                                                     max_length=result_length,\n",
    "                                                     num_beams=4,\n",
    "                                                     no_repeat_ngram_size=2,\n",
    "                                                     early_stopping=True\n",
    "                                                     )[0])\n",
    "\n",
    "    del inputs\n",
    "\n",
    "    found_labels = 0\n",
    "    found_label = \"\"\n",
    "\n",
    "    for label in list_of_labels:\n",
    "        if label in generated_text[generated_text.find('ANSWER')+3:].lower():\n",
    "            found_labels += 1\n",
    "            found_label = label\n",
    "\n",
    "    if found_labels == 1:\n",
    "        return found_label, generated_text\n",
    "    else:\n",
    "        return \"N/A\", generated_text\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"zero-shot/sib-200/\" + model_address[model_address.find('/')+1:] + \"/generate\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through subdirectories in the data directory\n",
    "for language in os.listdir(data_directory):\n",
    "# for language in ['sat_Olck', 'shn_Mymr']:\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'actual_category', 'predicted_category', 'generated_text'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, language)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Iterate through rows and compare predicted category with actual category\n",
    "            for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                actual_category = row['category']\n",
    "\n",
    "                # Predict category using your ZeroShot learning model\n",
    "                predicted_category, generated_text = predict_category(text)\n",
    "\n",
    "                # Append the results to the DataFrame\n",
    "                results_df = results_df.append({'text': text,\n",
    "                                                'actual_category': actual_category,\n",
    "                                                'predicted_category': 'science/technology' if predicted_category == 'science' else predicted_category,\n",
    "                                                'generated_text': generated_text}, ignore_index=True)\n",
    "                torch.cuda.empty_cache()\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{language}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T01:18:37.191522274Z",
     "start_time": "2024-02-01T01:17:57.383973190Z"
    }
   },
   "id": "89a087fa2a408ae6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/pan_Guru.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/hat_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tum_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kor_Hang.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tgk_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/slv_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/yue_Hant.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/sat_Olck.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kan_Knda.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ltg_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/nya_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/lij_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/yor_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/swh_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tuk_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ces_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/lug_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/bug_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ltz_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/grn_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tso_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/bho_Deva.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/spa_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tel_Telu.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ars_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/vec_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ceb_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/isl_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/sin_Sinh.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ban_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/pag_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/dan_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/nno_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kir_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ita_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/amh_Ethi.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/acm_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/lmo_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/sna_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/eng_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/scn_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/aeb_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/bod_Tibt.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/por_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/lus_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/bul_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/uig_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kmb_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/awa_Deva.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/arz_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/dik_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/nso_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/urd_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ydd_Hebr.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/zho_Hant.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kam_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/min_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/mai_Deva.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/fin_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/bel_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/slk_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/xho_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/hin_Deva.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/deu_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/gaz_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/fij_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/kab_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/nld_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/ckb_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/hne_Deva.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/uzn_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/khk_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/jav_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/arb_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/umb_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/gle_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/hye_Armn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/tpi_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/eus_Latn.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/prs_Arab.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/srp_Cyrl.csv\n",
      "Results saved to zero-shot/sib-200/bloom-3b/top_logprobs/sun_Latn.csv\n"
     ]
    }
   ],
   "source": [
    "# ZeroShot learning for SIB-200 dataset by using top logprobs\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
    "import warnings\n",
    "\n",
    "model_address = \"bigscience/bloom-3b\"\n",
    "# Filter out FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Assuming data directory contains multiple subdirectories with test.tsv files\n",
    "data_directory = \"sib-200/data/annotated\"\n",
    "\n",
    "if model_address.startswith(\"facebook\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = XGLMForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = XGLMTokenizer.from_pretrained(model_address)\n",
    "if model_address.startswith(\"bigscience\"):\n",
    "    # Load ZeroShot learning model and tokenizer\n",
    "    model = BloomForCausalLM.from_pretrained(model_address)\n",
    "    tokenizer = BloomTokenizerFast.from_pretrained(model_address)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_logprobs(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids, output_ids = inputs[\"input_ids\"], inputs[\"input_ids\"][:, 1:]\n",
    "    outputs = model(**inputs, labels=input_ids)\n",
    "    logits = outputs.logits\n",
    "    logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, output_ids.unsqueeze(2))\n",
    "    return logprobs\n",
    "\n",
    "\n",
    "def xglm_prediction(prompt, alternatives):\n",
    "    lprobs = [get_logprobs(prompt + \"\\n \" + alt).sum() for alt in alternatives]\n",
    "    return alternatives[lprobs.index(max(lprobs))]\n",
    "\n",
    "alternatives = [\"science\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", \"geography\"]\n",
    "\n",
    "# Output directory for saving DataFrames\n",
    "output_directory = \"zero-shot/sib-200/\" + model_address[model_address.find('/')+1:] + \"/top_logprobs\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through subdirectories in the data directory\n",
    "for language in os.listdir(data_directory)[122:]:\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=['text', 'actual_category', 'predicted_category'])\n",
    "\n",
    "    subdir = os.path.join(data_directory, language)\n",
    "    for file in os.listdir(subdir):\n",
    "        # Check if the file is a test.tsv file\n",
    "        if file.endswith(\"test.tsv\"):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Read the test.tsv file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Iterate through rows and compare predicted category with actual category\n",
    "            for index, row in df.iterrows():\n",
    "                text = row['text']\n",
    "                actual_category = row['category']\n",
    "\n",
    "                # Predict category using your ZeroShot learning model\n",
    "                with torch.no_grad():  # Disable gradient calculation\n",
    "                    predicted_category = xglm_prediction(text, alternatives)\n",
    "\n",
    "                # Append the results to the DataFrame\n",
    "                results_df = results_df.append({'text': text,\n",
    "                                                'actual_category': actual_category,\n",
    "                                                'predicted_category': 'science/technology' if predicted_category == 'science' else predicted_category}, ignore_index=True)\n",
    "                torch.cuda.empty_cache()\n",
    "                del text, actual_category, predicted_category\n",
    "            # Save the results DataFrame to a CSV file in the output directory\n",
    "            results_file_path = os.path.join(output_directory, f'{language}.csv')\n",
    "            results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "            print(f\"Results saved to {results_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:08:11.399428045Z",
     "start_time": "2024-01-28T08:49:17.462653431Z"
    }
   },
   "id": "66b5fce3c8077555",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(\"SIB-200 languages - ACL.xlsx\")\n",
    "\n",
    "# Calculte F1 score for each language and add it to the DataFrame\n",
    "def calculate_f1(language, model, method):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    results_df = pd.read_csv(f\"zero-shot/sib-200/{model}/{method}/{language}.csv\")\n",
    "\n",
    "    # Replace NaN values with 'N/A'\n",
    "    results_df = results_df.fillna('N/A')\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(results_df['actual_category'], results_df['predicted_category'], average='macro')\n",
    "\n",
    "    # Add the F1 score to the DataFrame\n",
    "    df.loc[df['Folder Name'] == language, f'F1 {model} {method}'] = f1\n",
    "\n",
    "# Iterate through languages and calculate F1 score for each language\n",
    "for language in df['Folder Name']:\n",
    "    calculate_f1(language, \"xglm-564M\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"xglm-1.7B\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"xglm-2.9B\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"xglm-7.5B\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloom-560M\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloom-1b1\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloom-1b7\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloom-3b\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloom-7b1\", \"top_logprobs\")\n",
    "    calculate_f1(language, \"bloomz-560M\", \"generate\")\n",
    "    calculate_f1(language, \"bloomz-1b1\", \"generate\")\n",
    "    calculate_f1(language, \"bloomz-1b7\", \"generate\")\n",
    "    calculate_f1(language, \"bloomz-3b\", \"generate\")\n",
    "    calculate_f1(language, \"bloomz-7b1\", \"generate\")\n",
    "\n",
    "# Save the updated DataFrame to the Excel file\n",
    "df.to_excel(\"SIB-200 languages - ACL.xlsx\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T17:44:24.276373630Z",
     "start_time": "2024-01-28T17:44:13.556646468Z"
    }
   },
   "id": "7a2b195881c57d00",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b21d6cf483e3847"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
